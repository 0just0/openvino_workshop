{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/openvino_start.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0: Register and Set Up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of the workshop is initializing the OpenVINO™ environment in this Jupyter notebook. \n",
    "The OpenVINO™ 2020.1 package have been installed to `intel/openvino/` already.\n",
    "\n",
    "To initialize the OpenVINO™ environment, run the `intel/openvino/bin/setupvars.sh` script.\n",
    "If the prerequisite steps have been done right, you will see the output: \n",
    "\n",
    "```\n",
    "[setupvars.sh] OpenVINO environment initialized\n",
    "OpenVINO Inference Engine version is: 2.1.37988\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ~/intel/openvino/bin/setupvars.sh\n",
    "\n",
    "from openvino import inference_engine as ie\n",
    "print('OpenVINO Inference Engine version: {}'.format(ie.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "##  1. Introduction\n",
    "\n",
    "##  2. What is SSD MobileNet V2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/training_vs_inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/about_vino.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostly for working with paths: os.path\n",
    "import os\n",
    "\n",
    "# working with arrays\n",
    "import numpy as np \n",
    "\n",
    "# path to data for the workshop\n",
    "WORKSHOP_DATA_PATH = os.path.join('.', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference in 4 Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once [OpenVINO™](https://docs.openvinotoolkit.org/) is installed, you can run an inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino import inference_engine as ie\n",
    "\n",
    "# Create an instance of the OpenVINO Inference Engine Core \n",
    "# This is the key module of the OpenVINO Inference Engine\n",
    "ie_core = ie.IECore()\n",
    "\n",
    "# Read a network from the Intermediate Representation (IR)\n",
    "network = ie.IENetwork(os.path.join(WORKSHOP_DATA_PATH, 'model.xml'), \n",
    "                       os.path.join(WORKSHOP_DATA_PATH, 'model.bin'))\n",
    "\n",
    "# Load the network that was read from the Intermediate Representation (IR) \n",
    "# to the CPU device \n",
    "network_loaded_on_device = ie_core.load_network(network=network, device_name='CPU')\n",
    "\n",
    "# Start an inference of the loaded network and return output data\n",
    "network_loaded_on_device.infer(inputs={'data': np.random.rand(1, 3, 227, 227)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, go to references of [OpenVINO Inference Engine Python API](https://docs.openvinotoolkit.org/latest/ie_python_api/annotated.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: What is SSD MobileNet V2?\n",
    "\n",
    "![](pictures/mobileNet-SSD-network-architecture.png)\n",
    "\n",
    "The `ssd_mobilenet_v2_coco` model is a [Single-Shot multibox Detection (SSD)](https://arxiv.org/pdf/1801.04381.pdf) network for object detection. The model has been trained from the Common Objects in Context (COCO) image dataset.\n",
    "\n",
    "The model input is a blob that consists of a single image of `1x3x300x300` in the `RGB` order.\n",
    "\n",
    "The model output is a typical vector containing the tracked object data. Note that the `class_id` data is now significant and should be used to determine the classification for any detected object.\n",
    "\n",
    "Model outputs:\n",
    "\n",
    "1. Classifier, name - `detection_classes`, contains predicted bounding boxes classes in range `[1, 91]`. The model was trained on Microsoft\\* COCO dataset version with 90 categories of objects.\n",
    "2. Probability, name - `detection_scores`, contains probability of detected bounding boxes.\n",
    "3. Detection box, name - `detection_boxes`, contains detection boxes coordinates in format `[y_min, x_min, y_max, x_max]`, where (`x_min`, `y_min`)  are coordinates top left corner, (`x_max`, `y_max`) are coordinates of the right bottom corner. Coordinates are rescaled to the input image size.\n",
    "4. Detections number, name - `num_detections`, contains the number of predicted detection boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Where Can I Find the Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the OpenVINO™ toolkit, you can easily download models from the [Intel&reg; Open Model Zoo](https://github.com/opencv/open_model_zoo).\n",
    "\n",
    "\n",
    "To see all available models (both public open-sourse from different frameworks (TensorFlow\\*, Caffe\\*, MxNet\\*, PyTorch\\* and others) and Intel&reg; ones), run the `downloader.py` script with the `--print_all` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to download an object-detection model called `ssd_mobilenet_v2_coco` using the [Model Downloader](https://github.com/opencv/open_model_zoo/tree/master/tools/downloader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/downloader.py \\\n",
    "--name ssd_mobilenet_v2_coco \\\n",
    "--output_dir ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Model Downloader can load not only publicly famous model, but also various models created at Intel for a range of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Downloader downloaded the model to the following directory: `data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Infer SSD MobileNet V2 on TensorFlow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.gfile import GFile\n",
    "\n",
    "# Path to the TensorFlow model\n",
    "model = os.path.join('data', 'public', 'ssd_mobilenet_v2_coco',\n",
    "                     'ssd_mobilenet_v2_coco_2018_03_29', 'frozen_inference_graph.pb')\n",
    "# SSD mobilenet v2 contains following output nodes\n",
    "output_names = ['detection_classes:0','detection_scores:0', 'detection_boxes:0', 'num_detections:0']\n",
    "\n",
    "# Create a graph\n",
    "graph = tf.Graph()\n",
    "# Create graph definitions\n",
    "graph_def = tf.GraphDef()\n",
    "\n",
    "# Read model to the graph definitions\n",
    "with open(model, \"rb\") as model_file:\n",
    "    graph_def.ParseFromString(model_file.read())\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "    # Import the graph definitions to TensorFlow\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "    # Get tensors for output nodes\n",
    "    output_tensors = [graph.get_tensor_by_name(layer_name) for layer_name in output_names] \n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        # Inference the model for random datates\n",
    "        print(session.run(output_tensors, feed_dict = {'image_tensor:0' : np.random.rand(1, 300, 300, 3)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Infer on Real Data on TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the TensorFlow `ssd_mobilenet_v2_coco` model, we need some utility functions and constant values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as log\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Initialize logging\n",
    "log.basicConfig(format=\"[ %(levelname)s ] %(message)s\", level=log.INFO, stream=sys.stdout)\n",
    "\n",
    "# Define how many times we run inference to get better performance\n",
    "NUM_RUNS = 1 \n",
    "# Number of images for one inference\n",
    "BATCH = 1\n",
    "\n",
    "# Contains all data for the workshop\n",
    "WORKSHOP_DATA_PATH = os.path.join('.', 'data')\n",
    "\n",
    "# Path to a test image\n",
    "IMAGE = os.path.join(WORKSHOP_DATA_PATH, 'images', 'input', 'cats.jpg')\n",
    "\n",
    "# Path to the downloaded TensorFlow image\n",
    "SSD_ASSETS = os.path.join(WORKSHOP_DATA_PATH, 'public', 'ssd_mobilenet_v2_coco')\n",
    "\n",
    "# Path to the downloaded frozen TensorFlow image\n",
    "TF_MODEL = os.path.join(SSD_ASSETS, 'ssd_mobilenet_v2_coco_2018_03_29', 'frozen_inference_graph.pb')\n",
    "\n",
    "# Path to the resulting TensorFlow image\n",
    "TF_RESULT_IMAGE = os.path.join(WORKSHOP_DATA_PATH, 'images', 'output', 'tensorflow_output.png')\n",
    "\n",
    "# Path to the Inference Engine FP32 model\n",
    "IE_MODEL_FP32_XML = os.path.join(SSD_ASSETS, 'FP32', 'ssd_mobilenet_v2_coco.xml')\n",
    "IE_MODEL_FP32_BIN = os.path.join(SSD_ASSETS, 'FP32', 'ssd_mobilenet_v2_coco.bin')\n",
    "\n",
    "# Path to the Inference Engine INT8 model optimized with the Default algorithm\n",
    "IE_MODEL_DEFAULT_INT8_XML = os.path.join(SSD_ASSETS, 'INT8', 'default', 'optimized', 'ssd_mobilenet_v2_coco.xml')\n",
    "IE_MODEL_DEFAULT_INT8_BIN = os.path.join(SSD_ASSETS, 'INT8', 'default', 'optimized', 'ssd_mobilenet_v2_coco.bin')\n",
    "\n",
    "# Path to the Inference Engine INT8 model optimized  with the AccuracyAware algorithm\n",
    "IE_MODEL_AA_INT8_XML = os.path.join(SSD_ASSETS, 'INT8', 'acuracy_aware', 'optimized', 'ssd_mobilenet_v2_coco.xml')\n",
    "IE_MODEL_AA_INT8_BIN = os.path.join(SSD_ASSETS, 'INT8', 'acuracy_aware', 'optimized', 'ssd_mobilenet_v2_coco.bin')\n",
    "\n",
    "# Path to the resulting TensorFlow image\n",
    "IE_RESULT_IMAGE = os.path.join(WORKSHOP_DATA_PATH, 'images', 'output', 'inference_engine_output.png')\n",
    "\n",
    "# Path to the combination of the resulting TensorFlow and Inference Engine images\n",
    "COMBO_RESULT_IMAGE = os.path.join(WORKSHOP_DATA_PATH, 'images', 'output', 'combo_output.png')\n",
    "\n",
    "PERFORMANCE = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenCV for image processing\n",
    "import cv2\n",
    "\n",
    "def read_resize_image(path_to_image: str, width: int, height: int):\n",
    "    \"\"\"\n",
    "    Takes an image and resizes it to the given dimensions.\n",
    "    \"\"\"\n",
    "    # Load the image \n",
    "    raw_image = cv2.imread(path_to_image)\n",
    "    # Return the image resized to the (width, height) format\n",
    "    return cv2.resize(raw_image, (width, height), interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required functions from TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "\n",
    "def tf_inference(graph: tf.Graph, input_data, input_name: str, outputs_names: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Returns TensorFlow model inference results.\n",
    "    \"\"\"\n",
    "    \n",
    "    log.info(\"Running inference with TensorFlow ...\")\n",
    "  \n",
    "    # Get the input tensor by name\n",
    "    input_tensor =  graph.get_tensor_by_name('{}:0'.format(input_name))\n",
    "    \n",
    "    # Fill input data\n",
    "    feed_dict = {\n",
    "        input_tensor: [input_data, ] \n",
    "    }\n",
    "\n",
    "    # Collect output tensors\n",
    "    output_tensors = []\n",
    "    \n",
    "    for output_name in outputs_names:\n",
    "        tensor = graph.get_tensor_by_name('{}:0'.format(output_name))\n",
    "        output_tensors.append(tensor)\n",
    "    \n",
    "    # Run inference and get performance\n",
    "    log.info(\"Running tf.Session\")\n",
    "    with graph.as_default():\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            inference_start = time.time()\n",
    "            outputs = session.run(output_tensors, feed_dict=feed_dict)\n",
    "            inference_end = time.time()\n",
    "    \n",
    "    # Collect inference results\n",
    "    res = dict(zip(outputs_names, outputs))\n",
    "    \n",
    "    log.info(\"TensorFlow reference collected successfully\")\n",
    "    \n",
    "    return res, inference_end - inference_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def tf_main(path_to_pb_model: str, \n",
    "            path_to_original_image: str, \n",
    "            number_inference: int = 1):\n",
    "    \"\"\"\n",
    "    Entrypoint to infer with TensorFlow.\n",
    "    \"\"\"\n",
    "    log.info('COMMON: image preprocessing')\n",
    "    \n",
    "    # Size of the image is 300x300 pixels, 3 channels in the RGB format\n",
    "    width = 300\n",
    "    \n",
    "    image_shape = (300, 300, 3)\n",
    "    \n",
    "    resized_image = read_resize_image(path_to_original_image, width, width)\n",
    "    \n",
    "    reshaped_image = np.reshape(resized_image, image_shape)\n",
    "    \n",
    "    log.info('Current shape: {}'.format(reshaped_image.shape))\n",
    "\n",
    "    log.info('TENSORFLOW SPECIFIC: Loading a model with TensorFlow')\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    graph = tf.Graph()\n",
    "    graph_def = tf.GraphDef()\n",
    "\n",
    "    with open(path_to_pb_model, \"rb\") as model_file:\n",
    "        graph_def.ParseFromString(model_file.read())\n",
    "\n",
    "    with graph.as_default():\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "    log.info(\"TensorFlow graph was created\")\n",
    "    \n",
    "    # We use SSD MobileNet V2 and we know the name of the input \n",
    "    input_layer = 'image_tensor'\n",
    "    \n",
    "    # And we know names of outputs\n",
    "    output_layers = ['num_detections', 'detection_classes', 'detection_scores', 'detection_boxes']\n",
    "    \n",
    "    collected_inference_time = []\n",
    "    \n",
    "    for run in range(number_inference):\n",
    "        raw_results, inference_time = tf_inference(graph, reshaped_image, input_layer, output_layers)\n",
    "        collected_inference_time.append(inference_time)\n",
    "    \n",
    "    tensorflow_average_inference_time = sum(collected_inference_time) / number_inference\n",
    "    \n",
    "    log.info('TENSORFLOW SPECIFIC: Plain inference finished')\n",
    "\n",
    "    return raw_results, tensorflow_average_inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Image class from display to show an image\n",
    "from IPython.display import Image\n",
    "# Show the image in the notebok\n",
    "Image(filename=IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer the Model on the Real Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework = 'TF'\n",
    "device = 'CPU'\n",
    "name = '{f} on {d}'.format(f=framework, d=device)\n",
    "\n",
    "tensorflow_fps_collected = []\n",
    "\n",
    "# Run inference on TensorFlow\n",
    "tensorflow_predictions, tensorflow_average_inference_time = tf_main(TF_MODEL, IMAGE, number_inference=NUM_RUNS)\n",
    "    \n",
    "log.info('Inference Time of SSD MobileNet V2 {} is {} seconds'.format(name, tensorflow_average_inference_time))\n",
    "\n",
    "# Calculate FPS from inference time\n",
    "tensorflow_average_fps = 1 / tensorflow_average_inference_time\n",
    "\n",
    "log.info('{} FPS: {}'.format(name, tensorflow_average_fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensorflow_predictions['num_detections']) # get number of detected objects\n",
    "print(tensorflow_predictions['detection_classes'][0])# get predicted classes IDs\n",
    "print(tensorflow_predictions['detection_scores'][0]) # get probabilities for predicted classes\n",
    "print(tensorflow_predictions['detection_boxes'][0]) # get boxes for predicted objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility functions to process images from TensorFlow and draw images\n",
    "from utils import parse_od_output, draw_image\n",
    "\n",
    "# Import the Image class from display to show an image\n",
    "from IPython.display import Image\n",
    "\n",
    "processd_tensorflow_predictions = parse_od_output(tensorflow_predictions)\n",
    "draw_image(IMAGE, processd_tensorflow_predictions, TF_RESULT_IMAGE)\n",
    "\n",
    "\n",
    "# Show the image in the notebok\n",
    "Image(filename=TF_RESULT_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: OpenVINO&trade; Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/openvino_toolkit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/additional_tools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Model Optimizer - Entry to OpenVINO&trade;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/model_optimizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's convert the TensorFlow model to the IR format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer/mo.py \\\n",
    "--output_dir=data/public/ssd_mobilenet_v2_coco/FP32 \\\n",
    "--reverse_input_channels \\\n",
    "--model_name=ssd_mobilenet_v2_coco \\\n",
    "--transformations_config=${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json \\\n",
    "--tensorflow_object_detection_api_pipeline_config=data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config \\\n",
    "--output=detection_classes,detection_scores,detection_boxes,num_detections \\\n",
    "--input_model=data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pictures/openvino_support.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the Intermediate Representation of the SSD MobileNet V2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/public/ssd_mobilenet_v2_coco/FP32/ssd_mobilenet_v2_coco.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 8: Inference of SSD MobileNet V2 on OpenVINO&trade; Inference Engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IECore, IENetwork\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def ie_inference(path_to_model_xml: str, path_to_model_bin: str, path_to_original_image: str, device='CPU', batch=1):\n",
    "    \"\"\"\n",
    "    Entrypoint to infer with the OpenVINO Inference Engine\n",
    "    \"\"\"\n",
    "\n",
    "    # Now let's create the IECore() entity \n",
    "    log.info(\"Creating Inference Engine Core\")   \n",
    "    ie = IECore()\n",
    "\n",
    "    # First, create a network (Note: you need to provide model in the IR previously converted with Model Optimizer)\n",
    "    log.info(\"Reading IR...\")\n",
    "    net = IENetwork(model=path_to_model_xml, weights=path_to_model_bin)\n",
    "\n",
    "    # Get input and output blob of the network\n",
    "    input_blob = next(iter(net.inputs))\n",
    "    out_blob = next(iter(net.outputs))\n",
    "\n",
    "    # Reshape the network to the needed batch\n",
    "    n, c, h, w = net.inputs[input_blob].shape\n",
    "    net.reshape({input_blob: (batch, c, h, w)})\n",
    "    n, c, h, w = net.inputs[input_blob].shape\n",
    "    \n",
    "    # Resize the image \n",
    "    log.info('COMMON: image preprocessing')\n",
    "    image = read_resize_image(path_to_original_image, h, w)\n",
    "    \n",
    "    # Now we load Network to the plugin\n",
    "    log.info(\"Loading IR to the plugin...\")\n",
    "    exec_net = ie.load_network(network=net, device_name=device, num_requests=2)\n",
    "\n",
    "    del net\n",
    "\n",
    "    labels_map = None\n",
    "    \n",
    "    # Read and preprocess the input image\n",
    "    image = image[..., ::-1]\n",
    "    in_frame = image.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "    batched_frame = np.array([in_frame for _ in range(batch)])\n",
    "    log.info('Current shape: {}'.format(batched_frame.shape))\n",
    "\n",
    "    # Now we run an inference on the target device\n",
    "    inference_start = time.time()\n",
    "    res = exec_net.infer(inputs={input_blob: batched_frame})\n",
    "    inference_end = time.time()\n",
    "\n",
    "    log.info('INFERENCE ENGINE SPECIFIC: no post-processing')\n",
    "\n",
    "    return res[out_blob], inference_end - inference_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_main(xml:str, bin:str, device:str, postfix: str = ''):\n",
    "    name = '{f} {p} on {d}'.format(f='IE', p=postfix, d=device)\n",
    "\n",
    "    inference_engine_fps_collected = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        # Run an inference on OpenVINO Inference Engine\n",
    "        predictions, inference_time = ie_inference(xml, bin,\n",
    "                                                   IMAGE,\n",
    "                                                   device,\n",
    "                                                   batch=BATCH)\n",
    "        \n",
    "        log.info('Inference Time of SSD MobileNet V2 {}: {}'.format(name, inference_time))\n",
    "        # Calculate FPS from inference time\n",
    "        inference_engine_fps = 1 / inference_time\n",
    "        \n",
    "        inference_engine_fps_collected.append(inference_engine_fps)\n",
    "\n",
    "    # Calculate the average FPS for all inferences\n",
    "    inference_engine_avg_fps = (sum(inference_engine_fps_collected) * BATCH) / (NUM_RUNS)\n",
    "    \n",
    "    PERFORMANCE[name] = inference_engine_avg_fps\n",
    "\n",
    "    log.info('{} FPS: {}'.format(name, inference_engine_avg_fps))\n",
    "    \n",
    "    return inference_engine_avg_fps, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'CPU'\n",
    "\n",
    "# Run the inference \n",
    "inference_engine_average_fps, inference_engine_predictions = ie_main(IE_MODEL_FP32_XML, \n",
    "                                                                     IE_MODEL_FP32_BIN, \n",
    "                                                                     device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_engine_predictions[0] # get data for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import parse_od_output, draw_image\n",
    "\n",
    "draw_image(IMAGE, inference_engine_predictions, IE_RESULT_IMAGE, color=(255, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Image class from display to show an image\n",
    "from IPython.display import Image\n",
    "\n",
    "# Show the image in the notebok\n",
    "Image(filename=IE_RESULT_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from Matplotlib to show barcharts\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "def show_results_interactively(tf_image: str, ie_image: str, combination_image: str, ie_fps:float, tf_fps:float):\n",
    "    \"\"\"\n",
    "    Takes paths to three images and shows them with Matplotlib on one screen.\n",
    "    \"\"\"\n",
    "    _ = plt.figure(figsize=(30, 10))\n",
    "    gs1 = gridspec.GridSpec(1, 3)\n",
    "    gs1.update(wspace=0.25, hspace=0.05)\n",
    "\n",
    "    titles = [\n",
    "        '(a) Tensorflow',\n",
    "        '(b) Inference Engine',\n",
    "        '(c) TensorFlow and Inference Engine\\n predictions are identical'\n",
    "    ]\n",
    "\n",
    "    for i, path in enumerate([tf_image, ie_image, combination_image]):\n",
    "        img_resized = cv2.imread(path)\n",
    "        ax_plot = plt.subplot(gs1[i])\n",
    "        ax_plot.axis(\"off\")\n",
    "        addon = ' '\n",
    "        if i == 1:\n",
    "            addon += '{:4.3f}'.format(ie_fps) + '(FPS)'\n",
    "        elif i == 0:\n",
    "            addon += '{:4.3f}'.format(tf_fps) + '(FPS)'\n",
    "\n",
    "        ax_plot.text(0.5, -0.5, titles[i] + addon,\n",
    "                     size=28, ha=\"center\",\n",
    "                     transform=ax_plot.transAxes)\n",
    "        ax_plot.imshow(cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import draw_image\n",
    "\n",
    "# Draw inference results from the Inference Engine in the image with TensorFlow inference results\n",
    "draw_image(TF_RESULT_IMAGE, inference_engine_predictions, COMBO_RESULT_IMAGE, color=(255, 0, 0))\n",
    "\n",
    "show_results_interactively(tf_image=TF_RESULT_IMAGE,\n",
    "                           ie_image=IE_RESULT_IMAGE,\n",
    "                           combination_image=COMBO_RESULT_IMAGE,\n",
    "                           ie_fps=inference_engine_average_fps,\n",
    "                           tf_fps=tensorflow_average_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_performance\n",
    "\n",
    "performance_data = {\n",
    "    'TF on CPU': tensorflow_average_fps,\n",
    "    'IE on CPU': inference_engine_average_fps\n",
    "}\n",
    "\n",
    "show_performance(performance_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oh, this is good - we got the same results in one image. But it is only ONE image. We need check accuracy on the whole dataset. So how can we do this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 9: [Accuracy Checker](https://github.com/opencv/open_model_zoo/tree/master/tools/accuracy_checker) - OpenVINO&trade; Accuracy Validation Framework\n",
    "\n",
    "![](pictures/accuracy_check.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace variables to the real path in the Accuracy Checker config:\n",
    "!WORKSHOP_PATH=$(pwd) envsubst '\\${WORKSHOP_PATH}' <data/configs/accuracy_checker_config_tf_template.yml >data/configs/accuracy_checker_config_tf.yml\n",
    "\n",
    "# Run the Accuracy Checker:\n",
    "!accuracy_check -c data/configs/accuracy_checker_config_tf.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Coffee Break***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace variables to the real path in the Accuracy Checker config:\n",
    "!WORKSHOP_PATH=$(pwd) envsubst '\\${WORKSHOP_PATH}' <data/configs/accuracy_checker_config_template.yml >data/configs/accuracy_checker_config.yml\n",
    "\n",
    "# Run the Accuracy Checker:\n",
    "!accuracy_check -c data/configs/accuracy_checker_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/configs/accuracy_checker_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 10: [Quantize the Model to Low Precision](https://docs.openvinotoolkit.org/latest/_compression_algorithms_quantization_README.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/quantization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/quantize.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 11: [Post-Trainig Optimization Toolkit](https://docs.openvinotoolkit.org/latest/_README.html)\n",
    "\n",
    "Post-Training Optimization Toolkit includes standalone command-line tool and Python* API that provide the following key features:\n",
    "\n",
    "* Two supported post-training quantization algorithms: fast [DefaultQuantization](https://docs.openvinotoolkit.org/latest/_compression_algorithms_quantization_default_README.html) and precise [AccuracyAwareQuantization](https://docs.openvinotoolkit.org/latest/_compression_algorithms_quantization_accuracy_aware_README.html).\n",
    "as well as multiple experimental methods including global optimization.\n",
    "* Symmetric and asymmetric quantization schemes. For more details, see the [Quantization](https://docs.openvinotoolkit.org/latest/_compression_algorithms_quantization_README.html) section.\n",
    "* Compression for different hardware targets such as CPU, GPU.\n",
    "* Per-channel quantization for Convolutional and Fully-Connected layers.\n",
    "* Multiple domains: Computer Vision, Recommendation Systems.\n",
    "* Ability to implement custom calibration pipeline via supported [API](https://docs.openvinotoolkit.org/latest/_sample_README.html).\n",
    "\n",
    "<br>   \n",
    "\n",
    "![](pictures/pot.png)\n",
    "\n",
    "<br>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/tools/post_training_optimization_toolkit/main.py \\\n",
    "-c data/configs/default/quantization_config.json \\\n",
    "--output-dir data/public/ssd_mobilenet_v2_coco/INT8/default \\\n",
    "--direct-dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat data/configs/default/quantization_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DefaultQuantization algorithm performs a fast but at the same time accurate INT8 calibration of NNs. It consists of three algorithms that are sequentially applied to a model:\n",
    "*  ActivationChannelAlignment - Used as a preliminary step before quantization and allows you to align ranges of output activations of Convolutional layers in order to reduce the quantization error.\n",
    "*  MinMaxQuantization - This is a vanilla quantization method that automatically inserts `FakeQuantize` operations into the model graph based on the specified  target hardware and initializes them\n",
    "using statistics collected on the calibration dataset.\n",
    "*  BiasCorrection - Adjusts biases of Convolutional and Fully-Connected layers based on the quantization error of the layer in order to make the overall error unbiased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'CPU'\n",
    "ie_avg_fps, predictions = ie_main(IE_MODEL_DEFAULT_INT8_XML, IE_MODEL_DEFAULT_INT8_BIN, device, 'INT8 D')\n",
    "\n",
    "draw_image(TF_RESULT_IMAGE, predictions, COMBO_RESULT_IMAGE, color=(0, 0, 255))\n",
    "\n",
    "show_results_interactively(tf_image=TF_RESULT_IMAGE,\n",
    "                           ie_image=IE_RESULT_IMAGE,\n",
    "                           combination_image=COMBO_RESULT_IMAGE,\n",
    "                           ie_fps=ie_avg_fps,\n",
    "                           tf_fps=tensorflow_average_fps)\n",
    "\n",
    "show_performance(PERFORMANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace variables to the real path in the Accuracy Checker config:\n",
    "!WORKSHOP_PATH=$(pwd) envsubst '\\${WORKSHOP_PATH}' <data/configs/default/accuracy_checker_config_template.yml >data/configs/default/accuracy_checker_config.yml\n",
    "\n",
    "# Run the Accuracy Checker\n",
    "!accuracy_check -c data/configs/default/accuracy_checker_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 12: AccuracyAware Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/tools/post_training_optimization_toolkit/main.py \\\n",
    "-c data/configs/accuracy_aware/quantization_config.json \\\n",
    "--output-dir data/public/ssd_mobilenet_v2_coco/INT8/acuracy_aware \\\n",
    "--direct-dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat data/configs/accuracy_aware/quantization_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The model gets fully quantized using the DefaultQuantization algorithm.\n",
    "2. The quantized and full-precision models are compared on a subset of the validation set in order to find mismatches in the target accuracy metric. A ranking subset is extracted based on the mismatches.\n",
    "3. A layer-wise ranking is performed in order to get a contribution of each quantized layer into the accuracy drop.\n",
    "4. Based on the ranking, the most \"problematic\" layer is reverted back to the original precision. This change is followed by the evaluation of the obtained model on the full validation set in order to get a new accuracy drop.\n",
    "5. If the accuracy criteria are satisfied for all pre-defined accuracy metrics, the algorithm finishes. Otherwise, it continues reverting the next \"problematic\" layer.\n",
    "6. It may happen that regular reverting does not get any accuracy improvement or even worsen the accuracy. Then the re-ranking is triggered as it is described in step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'CPU'\n",
    "ie_avg_fps, predictions = ie_main(IE_MODEL_AA_INT8_XML, IE_MODEL_AA_INT8_BIN, device, 'INT8 AA')\n",
    "\n",
    "draw_image(TF_RESULT_IMAGE, predictions, COMBO_RESULT_IMAGE, color=(0, 0, 255))\n",
    "\n",
    "show_results_interactively(tf_image=TF_RESULT_IMAGE,\n",
    "                           ie_image=IE_RESULT_IMAGE,\n",
    "                           combination_image=COMBO_RESULT_IMAGE,\n",
    "                           ie_fps=ie_avg_fps,\n",
    "                           tf_fps=tensorflow_average_fps)\n",
    "\n",
    "show_performance(PERFORMANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace variables to the real path in the Accuracy Checker config:\n",
    "!WORKSHOP_PATH=$(pwd) envsubst '\\${WORKSHOP_PATH}' <data/configs/accuracy_aware/accuracy_checker_config_template.yml >data/configs/accuracy_aware/accuracy_checker_config.yml\n",
    "\n",
    "# Run the Accuracy Checker\n",
    "!accuracy_check -c data/configs/accuracy_aware/accuracy_checker_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 13: [Deep Learning Boost](https://www.intel.ai/intel-deep-learning-boost/)\n",
    "\n",
    "![](pictures/dl_boost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 14: Get Even Better Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great performance results! But if you want the best performance, use C++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a C++ benchmark application in Inference samples. Let's build it from sources and try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ./data/samples/cpp/build \n",
    "! cd ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/samples/cpp/build && cmake .. && make benchmark_app -j8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/samples/cpp/build/intel64/Release/benchmark_app -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/samples/cpp/build/intel64/Release/benchmark_app -m data/public/ssd_mobilenet_v2_coco/FP32/ssd_mobilenet_v2_coco.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERFORMANCE['OpenVINO IE Benchmark INT8'] = 139.86\n",
    "show_performance(PERFORMANCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 15: Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Read/write video, work with images\n",
    "import cv2\n",
    "\n",
    "# Inference\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "\n",
    "# Show videos in the notebook\n",
    "from ipywidgets import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_XML = IE_MODEL_FP32_XML\n",
    "MODEL_PATH_BIN = IE_MODEL_FP32_BIN\n",
    "\n",
    "DEVICE = 'CPU'\n",
    "\n",
    "DATA_PATH = os.path.join('practice', 'data')\n",
    "INPUT_VIDEO = os.path.join(DATA_PATH, 'artyom.MP4')\n",
    "OUTPUT_VIDEO = os.path.join(DATA_PATH, 'out_artyom.MP4')\n",
    "\n",
    "LABELS_PATH = os.path.join(DATA_PATH, 'coco_labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video.from_file(INPUT_VIDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prapare_out_video_stream(input_video_stream):\n",
    "    width  = int(input_video_stream.get(3))\n",
    "    height = int(input_video_stream.get(4))\n",
    "    return cv2.VideoWriter(OUTPUT_VIDEO, cv2.VideoWriter_fourcc(*'X264'), 20, (width, height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object of IECore\n",
    "# This class represents an Inference Engine entity \n",
    "# and allows you to manipulate with plugins using unified interfaces\n",
    "ie = IECore()\n",
    "\n",
    "# Load network as Intermediate Representation \n",
    "# The IENetwork class contains information about the network model read from the Intermediate Representation\n",
    "# and allows you to manipulate with some model parameters such as layers affinity and output layers\n",
    "net = IENetwork(model=MODEL_PATH_XML, weights=MODEL_PATH_BIN)\n",
    "\n",
    "# Get names of input layers of the network\n",
    "input_blob = next(iter(net.inputs))\n",
    "\n",
    "print('Input layer of the network is {}'.format(input_blob))\n",
    "\n",
    "# Get shape (dimensions) of the input layer of the network\n",
    "# n - number of batches\n",
    "# c - number of an input image channels (usualy 3 - R, G and B) \n",
    "# h - height\n",
    "# w - width\n",
    "n, c, h, w = net.inputs[input_blob].shape\n",
    "\n",
    "print('Input shape of the network: [{}, {}, {}, {}]'.format(n, c, h, w))\n",
    "\n",
    "# Get names of output layers of the network\n",
    "out_blob = next(iter(net.outputs))\n",
    "\n",
    "print('Output layer of the network: {}'.format(out_blob))\n",
    "\n",
    "# Load names of COCO classes from the file \n",
    "with open(LABELS_PATH, 'r') as f:\n",
    "    labels_map = [x.strip() for x in f]\n",
    "\n",
    "# Load the network to the device\n",
    "# The load_network function returns an object of ExecutableNetwork\n",
    "# This class represents a network instance loaded to plugin and ready for inference\n",
    "exec_net = ie.load_network(network=net, num_requests=2, device_name=DEVICE)\n",
    "\n",
    "# Open an input video\n",
    "input_video_stream = cv2.VideoCapture(INPUT_VIDEO)\n",
    "\n",
    "# Create an output video stream\n",
    "out = prapare_out_video_stream(input_video_stream)\n",
    "\n",
    "feed_dict = {}\n",
    "\n",
    "cur_request_id = 0\n",
    "next_request_id = 1\n",
    "\n",
    "# Loop over frames in the input video\n",
    "while input_video_stream.isOpened():\n",
    "    \n",
    "    # Read the next frame from the intput video \n",
    "    ret, frame = input_video_stream.read()\n",
    "    # Check if the video is over\n",
    "    if not ret:\n",
    "        # Exit from the loop if the video is over\n",
    "        break \n",
    "    # Get height and width of the frame\n",
    "    frame_h, frame_w = frame.shape[:2]\n",
    "    \n",
    "    # Resize the frame to the network input \n",
    "    in_frame = cv2.resize(frame, (w, h))\n",
    "    \n",
    "    # Change the data layout from HWC to CHW\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  \n",
    "    \n",
    "    # Reshape the frame to the network input \n",
    "    in_frame = in_frame.reshape((n, c, h, w))\n",
    "    \n",
    "    # Prepare data for the network.\n",
    "    # This must be a dictionary: \n",
    "    #   key - name of the input layer\n",
    "    #   value - input data (the prepared frame)  \n",
    "    feed_dict[input_blob] = in_frame\n",
    "    \n",
    "    # Start asynchronous inference\n",
    "    # We must set request_id - number or identificator of the Inference Request\n",
    "    # and input data - the dictionary\n",
    "    exec_net.start_async(request_id=cur_request_id, inputs=feed_dict)\n",
    "    \n",
    "    # Wait the inference request until Inference Engine finishes the inference of the request\n",
    "    if exec_net.requests[cur_request_id].wait(-1) == 0:\n",
    "        # Read the result of the inference from the output layer of the execution network \n",
    "        inference_request_result = exec_net.requests[cur_request_id].outputs[out_blob]\n",
    "        \n",
    "        # Iterate over all discovered objects\n",
    "        for obj in inference_request_result[0][0]:\n",
    "            # Draw a bounding box only for objects the confidence of which is greater than a specified threshold\n",
    "            if obj[2] > 0.5:\n",
    "                # Get coordinates of the discovered object\n",
    "                # and scale it to the original size of the frame\n",
    "                xmin = int(obj[3] * frame_w)\n",
    "                ymin = int(obj[4] * frame_h)\n",
    "                xmax = int(obj[5] * frame_w)\n",
    "                ymax = int(obj[6] * frame_h)\n",
    "                \n",
    "                # Get class ID of the discovered object\n",
    "                class_id = int(obj[1])\n",
    "                \n",
    "                # Get confidence for the discovered object\n",
    "                confidence = round(obj[2] * 100, 1)\n",
    "                \n",
    "                # Draw a box and label\n",
    "                color = (min(class_id * 12.5, 255), min(class_id * 7, 255), min(class_id * 5, 255))\n",
    "                cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "                \n",
    "                # Get label of the class\n",
    "                label = labels_map[class_id]\n",
    "                \n",
    "                # Create the title of the object\n",
    "                text = '{}: {}% '.format(label, confidence)\n",
    "                \n",
    "                # Put the title to the frame\n",
    "                cv2.putText(frame, text, (xmin, ymin - 7), cv2.FONT_HERSHEY_COMPLEX, 2, color, 2)\n",
    "        \n",
    "    # Write the resulting frame to the output stream\n",
    "    out.write(frame)\n",
    "\n",
    "# Save the resulting video\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video.from_file(OUTPUT_VIDEO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
