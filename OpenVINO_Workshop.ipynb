{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OpenVINO: Демократизация оптимизации нейронных сетей\n",
    "![](pictures/openvino_start.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO Toolkit\n",
    "![](pictures/openvino_toolkit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of this workshop is initializing OpenVINO environment in this Jupyter notebook. \n",
    "The OpenVINO 2020.1 package have been installed to `intel/openvino/` already.\n",
    "For initializing the OpenVINO environment you should run the script `intel/openvino/bin/setupvars.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[setupvars.sh] OpenVINO environment initialized\r\n"
     ]
    }
   ],
   "source": [
    "!bash ~/intel/openvino/bin/setupvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Model Zoo\n",
    "![](pictures/models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenVINO package contains tools for easy download model from [OpenModelZoo](https://github.com/opencv/open_model_zoo) \n",
    "and convert the model to Intermediate Representation that OpenVINO supports\n",
    "\n",
    "To see all available models (both public open-sourse from original frameworks (TensorFlow, Caffe, MxNet, Pytorch e.t.c),\n",
    "and made in Intel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action-recognition-0001-decoder\r\n",
      "action-recognition-0001-encoder\r\n",
      "age-gender-recognition-retail-0013\r\n",
      "asl-recognition-0003\r\n",
      "driver-action-recognition-adas-0002-decoder\r\n",
      "driver-action-recognition-adas-0002-encoder\r\n",
      "emotions-recognition-retail-0003\r\n",
      "face-detection-adas-0001\r\n",
      "face-detection-adas-binary-0001\r\n",
      "face-detection-retail-0004\r\n",
      "face-detection-retail-0005\r\n",
      "face-reidentification-retail-0095\r\n",
      "facial-landmarks-35-adas-0002\r\n",
      "gaze-estimation-adas-0002\r\n",
      "handwritten-score-recognition-0003\r\n",
      "head-pose-estimation-adas-0001\r\n",
      "human-pose-estimation-0001\r\n",
      "image-retrieval-0001\r\n",
      "instance-segmentation-security-0010\r\n",
      "instance-segmentation-security-0050\r\n",
      "instance-segmentation-security-0083\r\n",
      "landmarks-regression-retail-0009\r\n",
      "license-plate-recognition-barrier-0001\r\n",
      "pedestrian-and-vehicle-detector-adas-0001\r\n",
      "pedestrian-detection-adas-0002\r\n",
      "pedestrian-detection-adas-binary-0001\r\n",
      "person-attributes-recognition-crossroad-0230\r\n",
      "person-detection-action-recognition-0005\r\n",
      "person-detection-action-recognition-0006\r\n",
      "person-detection-action-recognition-teacher-0002\r\n",
      "person-detection-asl-0001\r\n",
      "person-detection-raisinghand-recognition-0001\r\n",
      "person-detection-retail-0002\r\n",
      "person-detection-retail-0013\r\n",
      "person-reidentification-retail-0031\r\n",
      "person-reidentification-retail-0103\r\n",
      "person-reidentification-retail-0107\r\n",
      "person-reidentification-retail-0200\r\n",
      "person-vehicle-bike-detection-crossroad-0078\r\n",
      "person-vehicle-bike-detection-crossroad-1016\r\n",
      "product-detection-0001\r\n",
      "resnet18-xnor-binary-onnx-0001\r\n",
      "resnet50-binary-0001\r\n",
      "road-segmentation-adas-0001\r\n",
      "semantic-segmentation-adas-0001\r\n",
      "single-image-super-resolution-1032\r\n",
      "single-image-super-resolution-1033\r\n",
      "text-detection-0003\r\n",
      "text-detection-0004\r\n",
      "text-image-super-resolution-0001\r\n",
      "text-recognition-0012\r\n",
      "text-spotting-0001-detector\r\n",
      "text-spotting-0001-recognizer-decoder\r\n",
      "text-spotting-0001-recognizer-encoder\r\n",
      "vehicle-attributes-recognition-barrier-0039\r\n",
      "vehicle-detection-adas-0002\r\n",
      "vehicle-detection-adas-binary-0001\r\n",
      "vehicle-license-plate-detection-barrier-0106\r\n",
      "Sphereface\r\n",
      "alexnet\r\n",
      "brain-tumor-segmentation-0001\r\n",
      "brain-tumor-segmentation-0002\r\n",
      "caffenet\r\n",
      "ctdet_coco_dlav0_384\r\n",
      "ctdet_coco_dlav0_512\r\n",
      "ctpn\r\n",
      "deeplabv3\r\n",
      "densenet-121\r\n",
      "densenet-121-caffe2\r\n",
      "densenet-121-tf\r\n",
      "densenet-161\r\n",
      "densenet-161-tf\r\n",
      "densenet-169\r\n",
      "densenet-169-tf\r\n",
      "densenet-201\r\n",
      "efficientnet-b0\r\n",
      "efficientnet-b0-pytorch\r\n",
      "efficientnet-b0_auto_aug\r\n",
      "efficientnet-b5\r\n",
      "efficientnet-b5-pytorch\r\n",
      "efficientnet-b7-pytorch\r\n",
      "efficientnet-b7_auto_aug\r\n",
      "face-detection-retail-0044\r\n",
      "face-recognition-mobilefacenet-arcface\r\n",
      "face-recognition-resnet100-arcface\r\n",
      "face-recognition-resnet34-arcface\r\n",
      "face-recognition-resnet50-arcface\r\n",
      "facenet-20180408-102900\r\n",
      "faster_rcnn_inception_resnet_v2_atrous_coco\r\n",
      "faster_rcnn_inception_v2_coco\r\n",
      "faster_rcnn_resnet101_coco\r\n",
      "faster_rcnn_resnet50_coco\r\n",
      "googlenet-v1\r\n",
      "googlenet-v2\r\n",
      "googlenet-v3\r\n",
      "googlenet-v3-pytorch\r\n",
      "googlenet-v4\r\n",
      "human-pose-estimation-3d-0001\r\n",
      "inception-resnet-v2\r\n",
      "inception-resnet-v2-tf\r\n",
      "license-plate-recognition-barrier-0007\r\n",
      "mask_rcnn_inception_resnet_v2_atrous_coco\r\n",
      "mask_rcnn_inception_v2_coco\r\n",
      "mask_rcnn_resnet101_atrous_coco\r\n",
      "mask_rcnn_resnet50_atrous_coco\r\n",
      "mobilenet-ssd\r\n",
      "mobilenet-v1-0.25-128\r\n",
      "mobilenet-v1-0.50-160\r\n",
      "mobilenet-v1-0.50-224\r\n",
      "mobilenet-v1-1.0-224\r\n",
      "mobilenet-v1-1.0-224-tf\r\n",
      "mobilenet-v2\r\n",
      "mobilenet-v2-1.0-224\r\n",
      "mobilenet-v2-1.4-224\r\n",
      "mobilenet-v2-pytorch\r\n",
      "mtcnn-o\r\n",
      "mtcnn-p\r\n",
      "mtcnn-r\r\n",
      "octave-densenet-121-0.125\r\n",
      "octave-resnet-101-0.125\r\n",
      "octave-resnet-200-0.125\r\n",
      "octave-resnet-26-0.25\r\n",
      "octave-resnet-50-0.125\r\n",
      "octave-resnext-101-0.25\r\n",
      "octave-resnext-50-0.25\r\n",
      "octave-se-resnet-50-0.125\r\n",
      "resnet-101\r\n",
      "resnet-152\r\n",
      "resnet-50\r\n",
      "resnet-50-caffe2\r\n",
      "resnet-50-pytorch\r\n",
      "se-inception\r\n",
      "se-resnet-101\r\n",
      "se-resnet-152\r\n",
      "se-resnet-50\r\n",
      "se-resnext-101\r\n",
      "se-resnext-50\r\n",
      "single-human-pose-estimation-0001\r\n",
      "squeezenet1.0\r\n",
      "squeezenet1.1\r\n",
      "squeezenet1.1-caffe2\r\n",
      "ssd300\r\n",
      "ssd512\r\n",
      "ssd_mobilenet_v1_coco\r\n",
      "ssd_mobilenet_v1_fpn_coco\r\n",
      "ssd_mobilenet_v2_coco\r\n",
      "ssdlite_mobilenet_v2\r\n",
      "vehicle-license-plate-detection-barrier-0123\r\n",
      "vgg16\r\n",
      "vgg19\r\n",
      "vgg19-caffe2\r\n"
     ]
    }
   ],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For downloading any of these models you need to use downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: downloader.py [-h] [-c CONFIG.YML] [--name PAT[,PAT...]]\r\n",
      "                     [--list FILE.LST] [--all] [--print_all]\r\n",
      "                     [--precisions PREC[,PREC...]] [-o DIR] [--cache_dir DIR]\r\n",
      "                     [--num_attempts N] [--progress_format {text,json}]\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -c CONFIG.YML, --config CONFIG.YML\r\n",
      "                        model configuration file (deprecated)\r\n",
      "  --name PAT[,PAT...]   download only models whose names match at least one of\r\n",
      "                        the specified patterns\r\n",
      "  --list FILE.LST       download only models whose names match at least one of\r\n",
      "                        the patterns in the specified file\r\n",
      "  --all                 download all available models\r\n",
      "  --print_all           print all available models\r\n",
      "  --precisions PREC[,PREC...]\r\n",
      "                        download only models with the specified precisions\r\n",
      "                        (actual for DLDT networks)\r\n",
      "  -o DIR, --output_dir DIR\r\n",
      "                        path where to save models\r\n",
      "  --cache_dir DIR       directory to use as a cache for downloaded files\r\n",
      "  --num_attempts N      attempt each download up to N times\r\n",
      "  --progress_format {text,json}\r\n",
      "                        which format to use for progress reporting\r\n"
     ]
    }
   ],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/downloader.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to download an object detection model `ssd_mobilenet_v2_coco`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading models ||################\n",
      "\n",
      "========== Downloading data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco.tar.gz\n",
      "... 100%, 183521 KB, 6896 KB/s, 26 seconds passed\n",
      "\n",
      "################|| Post-processing ||################\n",
      "\n",
      "========== Unpacking data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!python3  ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/downloader.py \\\n",
    "--name ssd_mobilenet_v2_coco \\\n",
    "--output_dir ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Downloader has downloaded the model to `data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_<DATE>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\t\t\tmodel.ckpt.index  saved_model\r\n",
      "frozen_inference_graph.pb\tmodel.ckpt.meta\r\n",
      "model.ckpt.data-00000-of-00001\tpipeline.config\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the Model Downloader downloaded the model in TensorFlow format.\n",
    "You need convert this model to IR format. \n",
    "For this you need run converter script\n",
    "converter script runs the Model Optimizer with right parameters to converting the model with to IR.\n",
    "Of course  we can run the Model Optimizer directly. But for this we need pass right arguments to the Model Optimizer.\n",
    "All information about converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Converting ssd_mobilenet_v2_coco to IR (FP32)\n",
      "Conversion command: /home/atugarev/Developer/repositories/workbench/venv/bin/python3 -- /home/atugarev/intel/openvino_2020.1.023/deployment_tools/model_optimizer/mo.py --framework=tf --data_type=FP32 --output_dir=data/public/ssd_mobilenet_v2_coco/FP32 --model_name=ssd_mobilenet_v2_coco --reverse_input_channels '--input_shape=[1,300,300,3]' --input=image_tensor --transformations_config=/home/atugarev/intel/openvino_2020.1.023/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json --tensorflow_object_detection_api_pipeline_config=data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config --output=detection_classes,detection_scores,detection_boxes,num_detections --input_model=data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb\n",
      "\n",
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/atugarev/workshop/data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb\n",
      "\t- Path for generated IR: \t/home/atugarev/workshop/data/public/ssd_mobilenet_v2_coco/FP32\n",
      "\t- IR output name: \tssd_mobilenet_v2_coco\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \timage_tensor\n",
      "\t- Output layers: \tdetection_classes,detection_scores,detection_boxes,num_detections\n",
      "\t- Input shapes: \t[1,300,300,3]\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tTrue\n",
      "TensorFlow specific parameters:\n",
      "\t- Input model in text protobuf format: \tFalse\n",
      "\t- Path to model dump for TensorBoard: \tNone\n",
      "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
      "\t- Update the configuration file with input/output node names: \tNone\n",
      "\t- Use configuration file used to generate the model with Object Detection API: \t/home/atugarev/workshop/data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config\n",
      "\t- Operations to offload: \tNone\n",
      "\t- Patterns to offload: \tNone\n",
      "\t- Use the config file: \tNone\n",
      "Model Optimizer version: \t2020.1.0-61-gd349c3ba4a\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "The Preprocessor block has been removed. Only nodes performing mean value subtraction and scaling (if applicable) are kept.\n",
      "\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /home/atugarev/workshop/data/public/ssd_mobilenet_v2_coco/FP32/ssd_mobilenet_v2_coco.xml\n",
      "[ SUCCESS ] BIN file: /home/atugarev/workshop/data/public/ssd_mobilenet_v2_coco/FP32/ssd_mobilenet_v2_coco.bin\n",
      "[ SUCCESS ] Total execution time: 34.33 seconds. \n",
      "[ SUCCESS ] Memory consumed: 713 MB. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/converter.py \\\n",
    "--name ssd_mobilenet_v2_coco \\\n",
    "--download_dir ./data \\\n",
    "--output_dir ./data \\\n",
    "--precisions FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssd_mobilenet_v2_coco.bin      ssd_mobilenet_v2_coco.xml\r\n",
      "ssd_mobilenet_v2_coco.mapping\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/public/ssd_mobilenet_v2_coco/FP32/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find a command of running OpenVINO Model Optimizer in the output of the converter.py script.\n",
    "You can try this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/atugarev/workshop/data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb\n",
      "\t- Path for generated IR: \t/home/atugarev/workshop/data/public/ssd_mobilenet_v2_coco/FP32\n",
      "\t- IR output name: \tssd_mobilenet_v2_coco\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tdetection_classes,detection_scores,detection_boxes,num_detections\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tTrue\n",
      "TensorFlow specific parameters:\n",
      "\t- Input model in text protobuf format: \tFalse\n",
      "\t- Path to model dump for TensorBoard: \tNone\n",
      "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
      "\t- Update the configuration file with input/output node names: \tNone\n",
      "\t- Use configuration file used to generate the model with Object Detection API: \t/home/atugarev/workshop/data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config\n",
      "\t- Operations to offload: \tNone\n",
      "\t- Patterns to offload: \tNone\n",
      "\t- Use the config file: \tNone\n",
      "Model Optimizer version: \t2020.1.0-61-gd349c3ba4a\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "The Preprocessor block has been removed. Only nodes performing mean value subtraction and scaling (if applicable) are kept.\n",
      "\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /home/atugarev/workshop/data/public/ssd_mobilenet_v2_coco/FP32/ssd_mobilenet_v2_coco.xml\n",
      "[ SUCCESS ] BIN file: /home/atugarev/workshop/data/public/ssd_mobilenet_v2_coco/FP32/ssd_mobilenet_v2_coco.bin\n",
      "[ SUCCESS ] Total execution time: 33.55 seconds. \n",
      "[ SUCCESS ] Memory consumed: 707 MB. \n"
     ]
    }
   ],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer/mo.py \\\n",
    "--output_dir=data/public/ssd_mobilenet_v2_coco/FP32 \\\n",
    "--reverse_input_channels \\\n",
    "--model_name=ssd_mobilenet_v2_coco \\\n",
    "--transformations_config=${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json \\\n",
    "--tensorflow_object_detection_api_pipeline_config=data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config \\\n",
    "--output=detection_classes,detection_scores,detection_boxes,num_detections \\\n",
    "--input_model=data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Import OpenCV for image processing\n",
    "import cv2\n",
    "\n",
    "# Import some functions from matplotlib for show an image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Import needed functions from TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import graph_io\n",
    "\n",
    "# Import OpenVINO Inference Engine classes\n",
    "from openvino.inference_engine import IENetwork, IEPlugin, IECore\n",
    "\n",
    "# Import other needed functions\n",
    "import numpy as np\n",
    "\n",
    "import logging as log\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some function are needed for the next part of the workshop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_resize_image(path_to_image: str, width: int, height: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Takes an image and resizes it to the given dimensions\n",
    "    \"\"\"\n",
    "    #Load the image \n",
    "    raw_image = cv2.imread(path_to_image)\n",
    "    #Return the resized to (width, height) size image  \n",
    "    return cv2.resize(raw_image, (width, height), interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_performance(performance_data: dict):\n",
    "    \"\"\"\n",
    "    Takes dictionary contains name of configurations as keys and FPS for it as values\n",
    "    Plots bar chart with data\n",
    "    \"\"\"\n",
    "    l = np.arange(len(performance_data))\n",
    "    \n",
    "    performance = [fps for _, fps in performance_data.items()]\n",
    "    configurations = list(performance_data.keys())\n",
    "    figsize=(3*len(performance_data),10)\n",
    "    print(figsize)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    bars = ax.bar(x=l, height=performance, tick_label=configurations)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('#DDDDDD')\n",
    "    \n",
    "    ax.tick_params(bottom=False, left=False)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.yaxis.grid(True, color='#EEEEEE')\n",
    "    ax.xaxis.grid(False)\n",
    "   \n",
    "    bar_color = bars[0].get_facecolor()\n",
    "\n",
    "    for bar in bars:\n",
    "      ax.text(\n",
    "          bar.get_x() + bar.get_width() / 2,\n",
    "          bar.get_height() + 5,\n",
    "          round(bar.get_height(), 1),\n",
    "          horizontalalignment='center',\n",
    "          color=bar_color,\n",
    "          weight='bold',\n",
    "          fontsize=17\n",
    "      )\n",
    "    ax.set_xlabel('Configurations', labelpad=15, color='#333333')\n",
    "    ax.set_ylabel('Frame per seconds', labelpad=15, color='#333333')\n",
    "    ax.set_title('Performance mesuarments', pad=15, color='#333333', weight='bold')\n",
    "    plt.ylim(0, max(performance)+20)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(original_image: str,\n",
    "               res: tuple,\n",
    "               path_to_image: str,\n",
    "               prob_threshold: float=0.8,\n",
    "               color: tuple=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Takes a path to the image and bounding boxes. Draws those boxes on the new image and saves it\n",
    "    \"\"\"\n",
    "    raw_image = cv2.imread(original_image)\n",
    "    initial_w = raw_image.shape[1]\n",
    "    initial_h = raw_image.shape[0]\n",
    "    labels_map = {\n",
    "        18: 'dog',\n",
    "        21: 'cat'\n",
    "    }\n",
    "    for obj in res[0][0]:\n",
    "        # Draw only objects when probability more than specified threshold\n",
    "        if obj[2] > prob_threshold:\n",
    "            xmin = int(obj[3] * initial_w)\n",
    "            ymin = int(obj[4] * initial_h)\n",
    "            xmax = int(obj[5] * initial_w)\n",
    "            ymax = int(obj[6] * initial_h)\n",
    "            class_id = int(obj[1])\n",
    "            confidence = round(obj[2] * 100, 1)\n",
    "            cv2.rectangle(raw_image, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "            det_label = labels_map[class_id] if labels_map else str(class_id)\n",
    "            box_title = '{} {}%'.format(det_label, confidence)\n",
    "            cv2.putText(raw_image,\n",
    "                        box_title,\n",
    "                        (xmin, ymin - 7),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX, 5, color, cv2.LINE_AA)\n",
    "    cv2.imwrite(path_to_image, raw_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results_interactively(tf_image: str, ie_image: str, combination_image: str, ie_fps:float, tf_fps:float):\n",
    "    \"\"\"\n",
    "    Takes paths to three images and shows them with matplotlib on one screen\n",
    "    \"\"\"\n",
    "    _ = plt.figure(figsize=(30, 10))\n",
    "    gs1 = gridspec.GridSpec(1, 3)\n",
    "    gs1.update(wspace=0.25, hspace=0.05)\n",
    "\n",
    "    titles = [\n",
    "        '(a) Tensorflow',\n",
    "        '(b) Inference Engine',\n",
    "        '(c) TensorFlow and Inference Engine\\n predictions are identical'\n",
    "    ]\n",
    "\n",
    "    for i, path in enumerate([tf_image, ie_image, combination_image]):\n",
    "        img_resized = cv2.imread(path)\n",
    "        ax_plot = plt.subplot(gs1[i])\n",
    "        ax_plot.axis(\"off\")\n",
    "        addon = ' '\n",
    "        if i == 1:\n",
    "            addon += '{:4.3f}'.format(ie_fps) + '(FPS)'\n",
    "        elif i == 0:\n",
    "            addon += '{:4.3f}'.format(tf_fps) + '(FPS)'\n",
    "\n",
    "        ax_plot.text(0.5, -0.5, titles[i] + addon,\n",
    "                     size=28, ha=\"center\",\n",
    "                     transform=ax_plot.transAxes)\n",
    "        ax_plot.imshow(cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(path_to_model: str):\n",
    "    \"\"\"\n",
    "    Creates in memory graph in TensorFlow\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    graph = tf.Graph()\n",
    "    graph_def = tf.GraphDef()\n",
    "\n",
    "    with open(path_to_model, \"rb\") as model_file:\n",
    "        graph_def.ParseFromString(model_file.read())\n",
    "\n",
    "    nodes_to_clear_device = graph_def.node if isinstance(\n",
    "        graph_def, tf.GraphDef) else graph_def.graph_def.node\n",
    "    for node in nodes_to_clear_device:\n",
    "        node.device = \"\"\n",
    "\n",
    "    with graph.as_default():\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "    log.info(\"tf graph was created\")\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def children(op_name: str, graph: tf.Graph):\n",
    "    \"\"\"\n",
    "    Get operation node children\n",
    "    \"\"\"\n",
    "    op = graph.get_operation_by_name(op_name)\n",
    "    return set(op for out in op.outputs for op in out.consumers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_graph(graph_def) -> dict:\n",
    "    unlikely_output_types = [\n",
    "        'Const', 'Assign',\n",
    "        'NoOp', 'Placeholder',\n",
    "        'Assert', 'switch_t', 'switch_f'\n",
    "    ]\n",
    "    placeholders = dict()\n",
    "    outputs = list()\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():  # pylint: disable=not-context-manager\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "    for node in graph.as_graph_def().node:  # pylint: disable=no-member\n",
    "        if node.op == 'Placeholder':\n",
    "            node_dict = dict()\n",
    "            node_dict['type'] = tf.DType(node.attr['dtype'].type).name\n",
    "            new_shape = tf.TensorShape(node.attr['shape'].shape)\n",
    "            node_dict['shape'] = str(new_shape).replace(' ', '').replace('?', '-1')\n",
    "            placeholders[node.name] = node_dict\n",
    "        if len(children(node.name, graph)) == 0:\n",
    "            if node.op not in unlikely_output_types and \\\n",
    "                node.name.split('/')[-1] not in unlikely_output_types:\n",
    "                outputs.append(node.name)\n",
    "    result = dict()\n",
    "    result['inputs'] = placeholders\n",
    "    result['outputs'] = outputs\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_refs(graph: tf.Graph, input_data: dict):\n",
    "    \"\"\"\n",
    "    Return TensorFlow model reference results.\n",
    "    \"\"\"\n",
    "    log.info(\"Running inference with tensorflow ...\")\n",
    "    feed_dict = {}\n",
    "    summary_info = summarize_graph(graph.as_graph_def())\n",
    "    input_layers, output_layers = list(summary_info['inputs'].keys()), summary_info['outputs']\n",
    "\n",
    "    data_keys = [key for key in input_data.keys()]\n",
    "    if sorted(input_layers) != sorted(data_keys):\n",
    "        raise ValueError('input data keys: {0} do not match input '\n",
    "                         'layers of network: {1}'.format(data_keys, input_layers))\n",
    "\n",
    "    for input_layer_name in input_layers:\n",
    "        tensor = graph.get_tensor_by_name(input_layer_name + ':0')\n",
    "        feed_dict[tensor] = input_data[input_layer_name]\n",
    "    output_tensors = []\n",
    "    for name in output_layers:\n",
    "        tensor = graph.get_tensor_by_name(name + ':0')\n",
    "        output_tensors.append(tensor)\n",
    "\n",
    "    log.info(\"Running tf.Session\")\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # force inference on CPU\n",
    "    with graph.as_default():\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            inference_start = time.time()\n",
    "            outputs = session.run(output_tensors, feed_dict=feed_dict)\n",
    "            inference_end = time.time()\n",
    "    res = dict(zip(output_layers, outputs))\n",
    "    log.info(\"TensorFlow reference collected successfully\\n\")\n",
    "    return res, inference_end - inference_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_od_output(data: dict):\n",
    "    predictions = []\n",
    "    num_batches = len(data['detection_boxes'])\n",
    "    target_layers = ['num_detections', 'detection_classes',\n",
    "                     'detection_scores', 'detection_boxes']\n",
    "\n",
    "    for b in range(num_batches):\n",
    "        predictions.append([])\n",
    "        num_detections = int(data['num_detections'][b])\n",
    "        detection_classes = data['detection_classes'][b]\n",
    "        detection_scores = data['detection_scores'][b]\n",
    "        detection_boxes = data['detection_boxes'][b]\n",
    "        for i in range(num_detections):\n",
    "            obj = [\n",
    "                b, detection_classes[i], detection_scores[i],\n",
    "                detection_boxes[i][1], detection_boxes[i][0],\n",
    "                detection_boxes[i][3], detection_boxes[i][2]\n",
    "            ]\n",
    "            predictions[b].append(obj)\n",
    "    predictions = np.asarray(predictions)\n",
    "    new_shape = (1, 1, predictions.shape[0] * predictions.shape[1], predictions.shape[2])\n",
    "    predictions = np.reshape(predictions, newshape=new_shape)\n",
    "    parsed_data = {'tf_detections': predictions}\n",
    "    for layer, blob in data.items():\n",
    "        if layer not in target_layers:\n",
    "            parsed_data.update({layer: blob})\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_main(path_to_model: str, path_to_original_image: str, batch: int = 1):\n",
    "    \"\"\"\n",
    "    Entrypoint for inferencing with TensorFlow\n",
    "    \"\"\"\n",
    "    log.info('COMMON: image preprocessing')\n",
    "    width = 300\n",
    "    resized_image = read_resize_image(path_to_original_image, width, width)\n",
    "    reshaped_image = np.reshape(resized_image, (width, width, 3))\n",
    "    batched_image = np.array([reshaped_image for _ in range(batch)])\n",
    "    \n",
    "    log.info('Current shape: {}'.format(batched_image.shape))\n",
    "\n",
    "    log.info('TENSORFLOW SPECIFIC: Loading a model with TensorFLow')\n",
    "    graph = load_graph(path_to_model)\n",
    "\n",
    "    input_data = {\n",
    "        'image_tensor': batched_image,\n",
    "    }\n",
    "\n",
    "    raw_results, delta = get_refs(graph, input_data)\n",
    "    log.info('TENSORFLOW SPECIFIC: Plain inference finished')\n",
    "\n",
    "    log.info('TENSORFLOW SPECIFIC: Post processing started')\n",
    "    processed_results = parse_od_output(raw_results)\n",
    "    log.info('TENSORFLOW SPECIFIC: Post processing finished')\n",
    "\n",
    "    return processed_results['tf_detections'], delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_main(path_to_model_xml: str, path_to_model_bin: str, path_to_original_image: str, device='CPU', batch=1):\n",
    "    # First create Network (Note you need to provide model in IR previously converted with Model Optimizer)\n",
    "    log.info(\"Reading IR...\")\n",
    "    net = IENetwork(model=path_to_model_xml, weights=path_to_model_bin)\n",
    "\n",
    "    # Now let's create IECore() entity \n",
    "    log.info(\"Creating Inference Engine Core\")   \n",
    "    ie = IECore()\n",
    "\n",
    "\n",
    "    input_blob = next(iter(net.inputs))\n",
    "    out_blob = next(iter(net.outputs))\n",
    "\n",
    "    n, c, h, w = net.inputs[input_blob].shape\n",
    "    net.reshape({input_blob: (batch, c, h, w)})\n",
    "    n, c, h, w = net.inputs[input_blob].shape\n",
    "    \n",
    "    log.info('COMMON: image preprocessing')\n",
    "    image = read_resize_image(path_to_original_image, h, w)\n",
    "    # Now we load Network to plugin\n",
    "    log.info(\"Loading IR to the plugin...\")\n",
    "    exec_net = ie.load_network(network=net, device_name=device, num_requests=2)\n",
    "\n",
    "    del net\n",
    "\n",
    "    labels_map = None\n",
    "    \n",
    "    # Read and pre-process input image\n",
    "    image = image[..., ::-1]\n",
    "    in_frame = image.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "    batched_frame = np.array([in_frame for _ in range(batch)])\n",
    "    log.info('Current shape: {}'.format(batched_frame.shape))\n",
    "\n",
    "    # Now we run inference on target device\n",
    "    inference_start = time.time()\n",
    "    res = exec_net.infer(inputs={input_blob: batched_frame})\n",
    "    inference_end = time.time()\n",
    "\n",
    "    log.info('INFERENCE ENGINE SPECIFIC: no post processing')\n",
    "\n",
    "    return res[out_blob], inference_end - inference_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.basicConfig(format=\"[ %(levelname)s ] %(message)s\", level=log.INFO, stream=sys.stdout)\n",
    "\n",
    "NUM_RUNS = 1\n",
    "BATCH = 1\n",
    "\n",
    "DATA = os.path.join('.', 'data')\n",
    "\n",
    "IMAGE = os.path.join(DATA, 'images', 'input', 'dog.jpg')\n",
    "\n",
    "SSD_ASSETS = os.path.join(DATA, 'public', 'ssd_mobilenet_v2_coco')\n",
    "\n",
    "TF_MODEL = os.path.join(SSD_ASSETS, 'ssd_mobilenet_v2_coco_2018_03_29', 'frozen_inference_graph.pb')\n",
    "TF_RESULT_IMAGE = os.path.join(DATA, 'images', 'output', 'tensorflow_output.png')\n",
    "\n",
    "IE_MODEL_FP32_XML = os.path.join(SSD_ASSETS, 'FP32', 'ssd_mobilenet_v2_coco.xml')\n",
    "IE_MODEL_FP32_BIN = os.path.join(SSD_ASSETS, 'FP32', 'ssd_mobilenet_v2_coco.bin')\n",
    "\n",
    "IE_MODEL_DEFAULT_INT8_XML = os.path.join(SSD_ASSETS, 'INT8', 'default', 'optimized', 'ssd_mobilenet_v2_coco.xml')\n",
    "IE_MODEL_DEFAULT_INT8_BIN = os.path.join(SSD_ASSETS, 'INT8', 'default', 'optimized', 'ssd_mobilenet_v2_coco.bin')\n",
    "\n",
    "IE_MODEL_AA_INT8_XML = os.path.join(SSD_ASSETS, 'INT8', 'acuracy_aware', 'optimized', 'ssd_mobilenet_v2_coco.xml')\n",
    "IE_MODEL_AA_INT8_BIN = os.path.join(SSD_ASSETS, 'INT8', 'acuracy_aware', 'optimized', 'ssd_mobilenet_v2_coco.bin')\n",
    "\n",
    "\n",
    "IE_RESULT_IMAGE = os.path.join(DATA, 'images', 'output', 'inference_engine_output.png')\n",
    "\n",
    "OPENVINO = os.getenv('INTEL_OPENVINO_DIR')\n",
    "if not OPENVINO:\n",
    "    print('Please, install OpenVINO and initialize the environment')\n",
    "    sys.exit(1)\n",
    "\n",
    "COMBO_RESULT_IMAGE = os.path.join(DATA, 'images', 'output', 'combo_output.png')\n",
    "\n",
    "PERFORMANCE = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_inference(xml:str, bin:str, device:str, postfix: str):\n",
    "    name = '{f} {p} on {d}'.format(f='IE', p=postfix, d=device)\n",
    "\n",
    "    ie_fps_collected = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        predictions, inf_time = ie_main(xml, bin,\n",
    "                                        IMAGE,\n",
    "                                        device,\n",
    "                                        batch=BATCH)\n",
    "        ie_fps = 1 / inf_time\n",
    "        ie_fps_collected.append(ie_fps)\n",
    "\n",
    "    ie_avg_fps = (sum(ie_fps_collected) * BATCH) / (NUM_RUNS)\n",
    "\n",
    "    PERFORMANCE[name] = ie_avg_fps\n",
    "\n",
    "    log.info('{} FPS: {}'.format(name, ie_avg_fps))\n",
    "\n",
    "    draw_image(IMAGE, predictions, IE_RESULT_IMAGE, color=(0, 0, 255))\n",
    "    \n",
    "    return ie_avg_fps, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] COMMON: image preprocessing\n",
      "[ INFO ] Current shape: (1, 300, 300, 3)\n",
      "[ INFO ] TENSORFLOW SPECIFIC: Loading a model with TensorFLow\n",
      "[ INFO ] tf graph was created\n",
      "[ INFO ] Running inference with tensorflow ...\n",
      "[ INFO ] Running tf.Session\n",
      "[ INFO ] TensorFlow reference collected successfully\n",
      "\n",
      "[ INFO ] TENSORFLOW SPECIFIC: Plain inference finished\n",
      "[ INFO ] TENSORFLOW SPECIFIC: Post processing started\n",
      "[ INFO ] TENSORFLOW SPECIFIC: Post processing finished\n",
      "[ INFO ] TF on CPU FPS: 0.2070565933414952\n"
     ]
    }
   ],
   "source": [
    "framework = 'TF'\n",
    "device = 'CPU'\n",
    "name = '{f} on {d}'.format(f=framework, d=device)\n",
    "\n",
    "tf_fps_collected = []\n",
    "\n",
    "for i in range(NUM_RUNS):\n",
    "    predictions, inf_time = tf_main(TF_MODEL, \n",
    "                                    IMAGE,\n",
    "                                    batch=BATCH)\n",
    "    tf_fps = 1 / inf_time\n",
    "    tf_fps_collected.append(tf_fps)\n",
    "\n",
    "tf_avg_fps = (sum(tf_fps_collected) * BATCH) / (NUM_RUNS)\n",
    "\n",
    "PERFORMANCE[name] = tf_avg_fps\n",
    "\n",
    "log.info('{} FPS: {}'.format(name, tf_avg_fps))\n",
    "\n",
    "draw_image(IMAGE, predictions, TF_RESULT_IMAGE, color=(255, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accuracy_check -c data/configs/accuracy_checker_config_tf.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'CPU'\n",
    "ie_avg_fps, predictions = ie_inference(IE_MODEL_FP32_XML, IE_MODEL_FP32_BIN, device, '')\n",
    "\n",
    "draw_image(TF_RESULT_IMAGE, predictions, COMBO_RESULT_IMAGE, color=(0, 0, 255))\n",
    "\n",
    "show_results_interactively(tf_image=TF_RESULT_IMAGE,\n",
    "                           ie_image=IE_RESULT_IMAGE,\n",
    "                           combination_image=COMBO_RESULT_IMAGE,\n",
    "                           ie_fps=ie_avg_fps,\n",
    "                           tf_fps=tf_avg_fps)\n",
    "\n",
    "show_performance(PERFORMANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accuracy_check -c data/configs/accuracy_checker_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/tools/post_training_optimization_toolkit/main.py \\\n",
    "-c data/configs/default/quantization_config.json \\\n",
    "--output-dir data/public/ssd_mobilenet_v2_coco/INT8/default \\\n",
    "--direct-dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'CPU'\n",
    "ie_avg_fps, predictions = ie_inference(IE_MODEL_DEFAULT_INT8_XML, IE_MODEL_DEFAULT_INT8_BIN, device, 'INT8 D')\n",
    "\n",
    "draw_image(TF_RESULT_IMAGE, predictions, COMBO_RESULT_IMAGE, color=(0, 0, 255))\n",
    "\n",
    "show_results_interactively(tf_image=TF_RESULT_IMAGE,\n",
    "                           ie_image=IE_RESULT_IMAGE,\n",
    "                           combination_image=COMBO_RESULT_IMAGE,\n",
    "                           ie_fps=ie_avg_fps,\n",
    "                           tf_fps=tf_avg_fps)\n",
    "\n",
    "show_performance(PERFORMANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accuracy_check -c data/configs/default/accuracy_checker_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/tools/post_training_optimization_toolkit/main.py \\\n",
    "-c data/configs/accuracy_aware/quantization_config.json \\\n",
    "--output-dir data/public/ssd_mobilenet_v2_coco/INT8/acuracy_aware \\\n",
    "--direct-dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'CPU'\n",
    "ie_avg_fps, predictions = ie_inference(IE_MODEL_AA_INT8_XML, IE_MODEL_AA_INT8_BIN, device, 'INT8 AA')\n",
    "\n",
    "draw_image(TF_RESULT_IMAGE, predictions, COMBO_RESULT_IMAGE, color=(0, 0, 255))\n",
    "\n",
    "show_results_interactively(tf_image=TF_RESULT_IMAGE,\n",
    "                           ie_image=IE_RESULT_IMAGE,\n",
    "                           combination_image=COMBO_RESULT_IMAGE,\n",
    "                           ie_fps=ie_avg_fps,\n",
    "                           tf_fps=tf_avg_fps)\n",
    "\n",
    "show_performance(PERFORMANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accuracy_check -c data/configs/accuracy_aware/accuracy_checker_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/samples/cpp/build/intel64/Release/benchmark_app -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/samples/cpp/build/intel64/Release/benchmark_app -m workshop/data/public/ssd_mobilenet_v2_coco/FP32/ssd_mobilenet_v2_coco.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywebrtc\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/6b/b78b2b6f96fb2299eebae3083d3625cedb62a35ddc9815897f25c8093edd/ipywebrtc-0.5.0-py2.py3-none-any.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 305kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipywidgets>=7.4.0 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from ipywebrtc) (7.5.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from ipywidgets>=7.4.0->ipywebrtc) (3.5.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from ipywidgets>=7.4.0->ipywebrtc) (5.1.4)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from ipywidgets>=7.4.0->ipywebrtc) (7.11.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from ipywidgets>=7.4.0->ipywebrtc) (5.0.4)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from ipywidgets>=7.4.0->ipywebrtc) (4.3.3)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.4.0->ipywebrtc) (6.0.3)\n",
      "Requirement already satisfied: jupyter-client in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.4.0->ipywebrtc) (5.3.4)\n",
      "Requirement already satisfied: tornado>=4.2 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.4.0->ipywebrtc) (6.0.3)\n",
      "Requirement already satisfied: backcall in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.4.0->ipywebrtc) (0.1.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.4.0->ipywebrtc) (0.16.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.4.0->ipywebrtc) (3.0.3)\n",
      "Requirement already satisfied: pygments in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.4.0->ipywebrtc) (2.5.2)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.4.0->ipywebrtc) (4.8.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.4.0->ipywebrtc) (44.0.0)\n",
      "Requirement already satisfied: pickleshare in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.4.0->ipywebrtc) (0.7.5)\n",
      "Requirement already satisfied: decorator in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.4.0->ipywebrtc) (4.4.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets>=7.4.0->ipywebrtc) (3.2.0)\n",
      "Requirement already satisfied: ipython-genutils in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets>=7.4.0->ipywebrtc) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets>=7.4.0->ipywebrtc) (4.6.1)\n",
      "Requirement already satisfied: six in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from traitlets>=4.3.1->ipywidgets>=7.4.0->ipywebrtc) (1.13.0)\n",
      "Requirement already satisfied: nbconvert in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.4.0->ipywebrtc) (5.6.1)\n",
      "Requirement already satisfied: Send2Trash in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.4.0->ipywebrtc) (1.5.0)\n",
      "Requirement already satisfied: jinja2 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.4.0->ipywebrtc) (2.10.3)\n",
      "Requirement already satisfied: pyzmq>=17 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.4.0->ipywebrtc) (18.1.1)\n",
      "Requirement already satisfied: prometheus-client in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.4.0->ipywebrtc) (0.7.1)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.4.0->ipywebrtc) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.4.0->ipywebrtc) (2.8.1)\n",
      "Requirement already satisfied: parso>=0.5.2 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.4.0->ipywebrtc) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.4.0->ipywebrtc) (0.1.8)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.4.0->ipywebrtc) (0.6.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.4.0->ipywebrtc) (19.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.4.0->ipywebrtc) (0.15.7)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.4.0->ipywebrtc) (1.3.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.4.0->ipywebrtc) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.4.0->ipywebrtc) (0.6.0)\n",
      "Requirement already satisfied: testpath in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.4.0->ipywebrtc) (0.4.4)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.4.0->ipywebrtc) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.4.0->ipywebrtc) (0.3)\n",
      "Requirement already satisfied: bleach in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.4.0->ipywebrtc) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.4.0->ipywebrtc) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.4.0->ipywebrtc) (0.6.0)\n",
      "Requirement already satisfied: webencodings in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.4.0->ipywebrtc) (0.5.1)\n",
      "Requirement already satisfied: more-itertools in /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.4.0->ipywebrtc) (8.0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: ipywebrtc\n",
      "Successfully installed ipywebrtc-0.5.0\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywebrtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Read/write video, work with images\n",
    "import cv2\n",
    "\n",
    "# Inference\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "\n",
    "# Show videos in the notebook\n",
    "from ipywidgets import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_XML = 'data/public/ssd_mobilenet_v2_coco/FP32/ssd_mobilenet_v2_coco.xml'\n",
    "MODEL_PATH_BIN = 'data/public/ssd_mobilenet_v2_coco/FP32/ssd_mobilenet_v2_coco.bin'\n",
    "DEVICE = 'CPU'\n",
    "INPUT_VIDEO = 'practice/data/artyom.MP4'\n",
    "OUTPUT_VIDEO = 'practice/data/out_artyom.MP4'\n",
    "LABELS_PATH = 'practice/data/coco_labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e664c14df354700a6d5a151a358b0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Video(value=b'\\x00\\x00\\x00\\x18ftypmp42\\x00\\x00\\x00\\x01mp42avc1\\x00\\x02\\x0f\\xd0moov\\x00\\x00\\x00lmvhd\\x00\\x00\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Video.from_file(INPUT_VIDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prapare_out_video_stream(input_video_stream):\n",
    "    width  = int(input_video_stream.get(3))\n",
    "    height = int(input_video_stream.get(4))\n",
    "    return cv2.VideoWriter(OUTPUT_VIDEO, cv2.VideoWriter_fourcc(*'X264'), 20, (width, height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input layer of the network is image_tensor\n",
      "Input shape of the network is [1, 3, 300, 300]\n",
      "Output layer of the network is DetectionOutput\n"
     ]
    }
   ],
   "source": [
    "# Create object of IECore. \n",
    "# This class represents an Inference Engine entity \n",
    "# and allows you to manipulate with plugins using unified interfaces\n",
    "ie = IECore()\n",
    "\n",
    "# Load network as Intermediate Representation \n",
    "# The IENetwork class contains the information about the network model read from Intermediate Representation\n",
    "# and allows you to manipulate with some model parameters such as layers affinity and output layers\n",
    "net = IENetwork(model=MODEL_PATH_XML, weights=MODEL_PATH_BIN)\n",
    "\n",
    "# Get names of input layers of the network\n",
    "input_blob = next(iter(net.inputs))\n",
    "\n",
    "print('Input layer of the network is {}'.format(input_blob))\n",
    "\n",
    "# Get shape (dimensions) of the input layer of the network\n",
    "# n - number of batch\n",
    "# c - number of an input image channels (usualy 3 - R, G and B) \n",
    "# h - height\n",
    "# w - width\n",
    "n, c, h, w = net.inputs[input_blob].shape\n",
    "\n",
    "print('Input shape of the network is [{}, {}, {}, {}]'.format(n, c, h, w))\n",
    "\n",
    "# Get names of output layers of the network\n",
    "out_blob = next(iter(net.outputs))\n",
    "\n",
    "print('Output layer of the network is {}'.format(out_blob))\n",
    "\n",
    "# Load names of COCO classes from the file \n",
    "with open(LABELS_PATH, 'r') as f:\n",
    "    labels_map = [x.strip() for x in f]\n",
    "\n",
    "# Load the network to the device\n",
    "# The load_network function returns an object of ExecutableNetwork\n",
    "# This class represents a network instance loaded to plugin and ready for inference\n",
    "exec_net = ie.load_network(network=net, num_requests=2, device_name=DEVICE)\n",
    "\n",
    "\n",
    "# Open an input video\n",
    "input_video_stream = cv2.VideoCapture(INPUT_VIDEO)\n",
    "\n",
    "# Create an output video stream\n",
    "out = prapare_out_video_stream(input_video_stream)\n",
    "\n",
    "\n",
    "\n",
    "feed_dict = {}\n",
    "\n",
    "cur_request_id = 0\n",
    "next_request_id = 1\n",
    "\n",
    "# Do loop ny input video\n",
    "while input_video_stream.isOpened():\n",
    "    \n",
    "    # Read the next frame from the intput video \n",
    "    ret, frame = input_video_stream.read()\n",
    "    # Check if video is over\n",
    "    if not ret:\n",
    "        # Exit from the loop if video is over\n",
    "        break \n",
    "    # Get height and width of the frame\n",
    "    frame_h, frame_w = frame.shape[:2]\n",
    "    \n",
    "    # Resize the frame to network's input \n",
    "    in_frame = cv2.resize(frame, (w, h))\n",
    "    \n",
    "    # Change data layout from HWC to CHW\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  \n",
    "    \n",
    "    # Reshape the frame to network's input \n",
    "    in_frame = in_frame.reshape((n, c, h, w))\n",
    "    \n",
    "    # Prepare data for network.\n",
    "    # This must be a dictionary: \n",
    "    #   key - name of the input layer\n",
    "    #   value - input data (the prepared frame)  \n",
    "    feed_dict[input_blob] = in_frame\n",
    "    \n",
    "    # Start Asynchronous Inference.\n",
    "    # We must set request_id - number or identificator of Inference Request\n",
    "    # and input data - the dictionary\n",
    "    exec_net.start_async(request_id=cur_request_id, inputs=feed_dict)\n",
    "    \n",
    "    # Wait the inference request until Inference Engine finished the inference of the request\n",
    "    if exec_net.requests[cur_request_id].wait(-1) == 0:\n",
    "        # Read result of the inference from the out layer of the execution network \n",
    "        inference_request_result = exec_net.requests[cur_request_id].outputs[out_blob]\n",
    "        \n",
    "        # Iterate by all found objects\n",
    "        for obj in inference_request_result[0][0]:\n",
    "            # Draw only objects when probability more than specified threshold\n",
    "            if obj[2] > 0.5:\n",
    "                # Get coordinates of the found object\n",
    "                # and scale it to the original size of the frame\n",
    "                xmin = int(obj[3] * frame_w)\n",
    "                ymin = int(obj[4] * frame_h)\n",
    "                xmax = int(obj[5] * frame_w)\n",
    "                ymax = int(obj[6] * frame_h)\n",
    "                \n",
    "                # Get class ID of the found object\n",
    "                class_id = int(obj[1])\n",
    "                \n",
    "                # Get confidence for the found object.\n",
    "                confidence = round(obj[2] * 100, 1)\n",
    "                \n",
    "                # Draw box and label\n",
    "                color = (min(class_id * 12.5, 255), min(class_id * 7, 255), min(class_id * 5, 255))\n",
    "                cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "                \n",
    "                # Get label of the class\n",
    "                label = labels_map[class_id]\n",
    "                \n",
    "                # Create titel of the object\n",
    "                text = '{}: {}% '.format(label, confidence)\n",
    "                \n",
    "                # Put the titel to the frame\n",
    "                cv2.putText(frame, text, (xmin, ymin - 7), cv2.FONT_HERSHEY_COMPLEX, 2, color, 2)\n",
    "        \n",
    "    # Write the result frame to the out stream\n",
    "    out.write(frame)\n",
    "\n",
    "# Save result video\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31f2764599d4745b9a0769ac9ad6f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Video(value=b'\\x00\\x00\\x00 ftypisom\\x00\\x00\\x02\\x00isomiso2avc1mp41\\x00\\x00\\x00\\x08free\\x00_AWmdat\\x00\\x00\\x02…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Video.from_file(OUTPUT_VIDEO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
