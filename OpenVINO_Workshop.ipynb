{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OpenVINO: Демократизация оптимизации нейронных сетей\n",
    "![](pictures/openvino_start.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO Toolkit\n",
    "![](pictures/openvino_toolkit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of this workshop is initializing OpenVINO environment in this Jupyter notebook. \n",
    "The OpenVINO 2020.1 package have been installed to `intel/openvino/` already.\n",
    "For initializing the OpenVINO environment you should run the script `intel/openvino/bin/setupvars.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash intel/openvino/bin/setupvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to check the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo LD_LIBRARY_PATH is $LD_LIBRARY_PATH\n",
    "!echo\n",
    "!echo PYTHONPATH is $PYTHONPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are paths to the OpenVINO in LD_LIBRARY_PATH and PYTHONPATH variables.\n",
    "So you can already use the OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Model Zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](workshop/pictures/models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenVINO package contains tools for easy download model from [OpenModelZoo](https://github.com/opencv/open_model_zoo) \n",
    "and convert the model to Intermediate Representation that OpenVINO supports\n",
    "\n",
    "To see all available models (both public open-sourse from original frameworks (TensorFlow, Caffe, MxNet, Pytorch e.t.c),\n",
    "and made in Intel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action-recognition-0001-decoder\r\n",
      "action-recognition-0001-encoder\r\n",
      "age-gender-recognition-retail-0013\r\n",
      "asl-recognition-0003\r\n",
      "driver-action-recognition-adas-0002-decoder\r\n",
      "driver-action-recognition-adas-0002-encoder\r\n",
      "emotions-recognition-retail-0003\r\n",
      "face-detection-adas-0001\r\n",
      "face-detection-adas-binary-0001\r\n",
      "face-detection-retail-0004\r\n",
      "face-detection-retail-0005\r\n",
      "face-reidentification-retail-0095\r\n",
      "facial-landmarks-35-adas-0002\r\n",
      "gaze-estimation-adas-0002\r\n",
      "handwritten-score-recognition-0003\r\n",
      "head-pose-estimation-adas-0001\r\n",
      "human-pose-estimation-0001\r\n",
      "image-retrieval-0001\r\n",
      "instance-segmentation-security-0010\r\n",
      "instance-segmentation-security-0050\r\n",
      "instance-segmentation-security-0083\r\n",
      "landmarks-regression-retail-0009\r\n",
      "license-plate-recognition-barrier-0001\r\n",
      "pedestrian-and-vehicle-detector-adas-0001\r\n",
      "pedestrian-detection-adas-0002\r\n",
      "pedestrian-detection-adas-binary-0001\r\n",
      "person-attributes-recognition-crossroad-0230\r\n",
      "person-detection-action-recognition-0005\r\n",
      "person-detection-action-recognition-0006\r\n",
      "person-detection-action-recognition-teacher-0002\r\n",
      "person-detection-asl-0001\r\n",
      "person-detection-raisinghand-recognition-0001\r\n",
      "person-detection-retail-0002\r\n",
      "person-detection-retail-0013\r\n",
      "person-reidentification-retail-0031\r\n",
      "person-reidentification-retail-0103\r\n",
      "person-reidentification-retail-0107\r\n",
      "person-reidentification-retail-0200\r\n",
      "person-vehicle-bike-detection-crossroad-0078\r\n",
      "person-vehicle-bike-detection-crossroad-1016\r\n",
      "product-detection-0001\r\n",
      "resnet18-xnor-binary-onnx-0001\r\n",
      "resnet50-binary-0001\r\n",
      "road-segmentation-adas-0001\r\n",
      "semantic-segmentation-adas-0001\r\n",
      "single-image-super-resolution-1032\r\n",
      "single-image-super-resolution-1033\r\n",
      "text-detection-0003\r\n",
      "text-detection-0004\r\n",
      "text-image-super-resolution-0001\r\n",
      "text-recognition-0012\r\n",
      "text-spotting-0001-detector\r\n",
      "text-spotting-0001-recognizer-decoder\r\n",
      "text-spotting-0001-recognizer-encoder\r\n",
      "vehicle-attributes-recognition-barrier-0039\r\n",
      "vehicle-detection-adas-0002\r\n",
      "vehicle-detection-adas-binary-0001\r\n",
      "vehicle-license-plate-detection-barrier-0106\r\n",
      "Sphereface\r\n",
      "alexnet\r\n",
      "brain-tumor-segmentation-0001\r\n",
      "brain-tumor-segmentation-0002\r\n",
      "caffenet\r\n",
      "ctdet_coco_dlav0_384\r\n",
      "ctdet_coco_dlav0_512\r\n",
      "ctpn\r\n",
      "deeplabv3\r\n",
      "densenet-121\r\n",
      "densenet-121-caffe2\r\n",
      "densenet-121-tf\r\n",
      "densenet-161\r\n",
      "densenet-161-tf\r\n",
      "densenet-169\r\n",
      "densenet-169-tf\r\n",
      "densenet-201\r\n",
      "efficientnet-b0\r\n",
      "efficientnet-b0-pytorch\r\n",
      "efficientnet-b0_auto_aug\r\n",
      "efficientnet-b5\r\n",
      "efficientnet-b5-pytorch\r\n",
      "efficientnet-b7-pytorch\r\n",
      "efficientnet-b7_auto_aug\r\n",
      "face-detection-retail-0044\r\n",
      "face-recognition-mobilefacenet-arcface\r\n",
      "face-recognition-resnet100-arcface\r\n",
      "face-recognition-resnet34-arcface\r\n",
      "face-recognition-resnet50-arcface\r\n",
      "facenet-20180408-102900\r\n",
      "faster_rcnn_inception_resnet_v2_atrous_coco\r\n",
      "faster_rcnn_inception_v2_coco\r\n",
      "faster_rcnn_resnet101_coco\r\n",
      "faster_rcnn_resnet50_coco\r\n",
      "googlenet-v1\r\n",
      "googlenet-v2\r\n",
      "googlenet-v3\r\n",
      "googlenet-v3-pytorch\r\n",
      "googlenet-v4\r\n",
      "human-pose-estimation-3d-0001\r\n",
      "inception-resnet-v2\r\n",
      "inception-resnet-v2-tf\r\n",
      "license-plate-recognition-barrier-0007\r\n",
      "mask_rcnn_inception_resnet_v2_atrous_coco\r\n",
      "mask_rcnn_inception_v2_coco\r\n",
      "mask_rcnn_resnet101_atrous_coco\r\n",
      "mask_rcnn_resnet50_atrous_coco\r\n",
      "mobilenet-ssd\r\n",
      "mobilenet-v1-0.25-128\r\n",
      "mobilenet-v1-0.50-160\r\n",
      "mobilenet-v1-0.50-224\r\n",
      "mobilenet-v1-1.0-224\r\n",
      "mobilenet-v1-1.0-224-tf\r\n",
      "mobilenet-v2\r\n",
      "mobilenet-v2-1.0-224\r\n",
      "mobilenet-v2-1.4-224\r\n",
      "mobilenet-v2-pytorch\r\n",
      "mtcnn-o\r\n",
      "mtcnn-p\r\n",
      "mtcnn-r\r\n",
      "octave-densenet-121-0.125\r\n",
      "octave-resnet-101-0.125\r\n",
      "octave-resnet-200-0.125\r\n",
      "octave-resnet-26-0.25\r\n",
      "octave-resnet-50-0.125\r\n",
      "octave-resnext-101-0.25\r\n",
      "octave-resnext-50-0.25\r\n",
      "octave-se-resnet-50-0.125\r\n",
      "resnet-101\r\n",
      "resnet-152\r\n",
      "resnet-50\r\n",
      "resnet-50-caffe2\r\n",
      "resnet-50-pytorch\r\n",
      "se-inception\r\n",
      "se-resnet-101\r\n",
      "se-resnet-152\r\n",
      "se-resnet-50\r\n",
      "se-resnext-101\r\n",
      "se-resnext-50\r\n",
      "single-human-pose-estimation-0001\r\n",
      "squeezenet1.0\r\n",
      "squeezenet1.1\r\n",
      "squeezenet1.1-caffe2\r\n",
      "ssd300\r\n",
      "ssd512\r\n",
      "ssd_mobilenet_v1_coco\r\n",
      "ssd_mobilenet_v1_fpn_coco\r\n",
      "ssd_mobilenet_v2_coco\r\n",
      "ssdlite_mobilenet_v2\r\n",
      "vehicle-license-plate-detection-barrier-0123\r\n",
      "vgg16\r\n",
      "vgg19\r\n",
      "vgg19-caffe2\r\n"
     ]
    }
   ],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For downloading any of these models you need to use downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: downloader.py [-h] [-c CONFIG.YML] [--name PAT[,PAT...]]\r\n",
      "                     [--list FILE.LST] [--all] [--print_all]\r\n",
      "                     [--precisions PREC[,PREC...]] [-o DIR] [--cache_dir DIR]\r\n",
      "                     [--num_attempts N] [--progress_format {text,json}]\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -c CONFIG.YML, --config CONFIG.YML\r\n",
      "                        model configuration file (deprecated)\r\n",
      "  --name PAT[,PAT...]   download only models whose names match at least one of\r\n",
      "                        the specified patterns\r\n",
      "  --list FILE.LST       download only models whose names match at least one of\r\n",
      "                        the patterns in the specified file\r\n",
      "  --all                 download all available models\r\n",
      "  --print_all           print all available models\r\n",
      "  --precisions PREC[,PREC...]\r\n",
      "                        download only models with the specified precisions\r\n",
      "                        (actual for DLDT networks)\r\n",
      "  -o DIR, --output_dir DIR\r\n",
      "                        path where to save models\r\n",
      "  --cache_dir DIR       directory to use as a cache for downloaded files\r\n",
      "  --num_attempts N      attempt each download up to N times\r\n",
      "  --progress_format {text,json}\r\n",
      "                        which format to use for progress reporting\r\n"
     ]
    }
   ],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/downloader.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to download an object detection model `ssd_mobilenet_v2_coco`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3  ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/downloader.py \\\n",
    "                                                                                --name ssd_mobilenet_v2_coco \\\n",
    "                                                                                --output_dir ./workshop/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Downloader has downloaded the model to `./workshop/data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_<DATE>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la ./workshop/data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_<DATE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the Model Downloader downloaded the model in TensorFlow format.\n",
    "You need convert this model to IR format. \n",
    "For this you need run converter script\n",
    "converter script runs the Model Optimizer with right parameters to converting the model with to IR.\n",
    "Of course  we can run the Model Optimizer directly. But for this we need pass right arguments to the Model Optimizer.\n",
    "All information about converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/converter.py \\\n",
    "                                                         --name ssd_mobilenet_v2_coco \\\n",
    "                                                         --download_dir ./workshop/data \\\n",
    "                                                         --output_dir ./workshop/data \\\n",
    "                                                         --precisions FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la ./workshop/data/public/ssd_mobilenet_v2_coco/FP32/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find a command of running OpenVINO Model Optimizer in the output of the converter.py script.\n",
    "You can try this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer/mo.py \\\n",
    "    --output_dir=./workshop/data/public/ssd_mobilenet_v2_coco/FP32 \\\n",
    "    --reverse_input_channels \\\n",
    "    --model_name=ssd_mobilenet_v2_coco \\\n",
    "    --transformations_config=${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json \\\n",
    "    --tensorflow_object_detection_api_pipeline_config=./workshop/data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config \\\n",
    "    --output=detection_classes,detection_scores,detection_boxes,num_detections \\\n",
    "    --input_model=./workshop/data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some function are needed for the next part of the workshop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def read_resize_image(path_to_image: str, width: int, height: int):\n",
    "    \"\"\"\n",
    "    Takes an image and resizes it to the given dimensions\n",
    "    \"\"\"\n",
    "    #Load the image \n",
    "    raw_image = cv2.imread(path_to_image)\n",
    "    #Return the resized to (width, height) size image  \n",
    "    return cv2.resize(raw_image, (width, height), interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_performance(performance_data: dict):\n",
    "    \"\"\"\n",
    "    Takes dictionary contains name of configurations as keys and FPS for it as values\n",
    "    \"\"\"\n",
    "    y_pos = np.arange(len(performance_data))\n",
    "    performance = [fps for case, fps in performance_data.items()]\n",
    "    plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, performance_data.keys())\n",
    "    plt.ylabel('FPS')\n",
    "    plt.title('Configurations')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(original_image: str, res: tuple, path_to_image: str, prob_threshold: float=0.8, color: tuple=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Takes a path to the image and bounding boxes. Draws those boxes on the new image and saves it\n",
    "    \"\"\"\n",
    "    raw_image = cv2.imread(original_image)\n",
    "    initial_w = raw_image.shape[1]\n",
    "    initial_h = raw_image.shape[0]\n",
    "    labels_map = {\n",
    "        18: 'dog',\n",
    "        21: 'cat'\n",
    "    }\n",
    "    for obj in res[0][0]:\n",
    "        # Draw only objects when probability more than specified threshold\n",
    "        if obj[2] > prob_threshold:\n",
    "            xmin = int(obj[3] * initial_w)\n",
    "            ymin = int(obj[4] * initial_h)\n",
    "            xmax = int(obj[5] * initial_w)\n",
    "            ymax = int(obj[6] * initial_h)\n",
    "            class_id = int(obj[1])\n",
    "            confidence = round(obj[2] * 100, 1)\n",
    "            cv2.rectangle(raw_image, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "            det_label = labels_map[class_id] if labels_map else str(class_id)\n",
    "            box_title = '{} {}%'.format(det_label, confidence)\n",
    "            cv2.putText(raw_image,\n",
    "                        box_title,\n",
    "                        (xmin, ymin - 7),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX, 5, color, cv2.LINE_AA)\n",
    "    cv2.imwrite(path_to_image, raw_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "def show_results_interactively(tf_image, ie_image, combination_image, ie_fps, tf_fps):\n",
    "    \"\"\"\n",
    "    Takes paths to three images and shows them with matplotlib on one screen\n",
    "    \"\"\"\n",
    "    _ = plt.figure(figsize=(30, 10))\n",
    "    gs1 = gridspec.GridSpec(1, 3)\n",
    "    gs1.update(wspace=0.25, hspace=0.05)\n",
    "\n",
    "    titles = [\n",
    "        '(a) Tensorflow',\n",
    "        '(b) Inference Engine',\n",
    "        '(c) TensorFlow and Inference Engine\\n predictions are identical'\n",
    "    ]\n",
    "\n",
    "    for i, path in enumerate([tf_image, ie_image, combination_image]):\n",
    "        img_resized = cv2.imread(path)\n",
    "        ax_plot = plt.subplot(gs1[i])\n",
    "        ax_plot.axis(\"off\")\n",
    "        addon = ' '\n",
    "        if i == 1:\n",
    "            addon += '{:4.3f}'.format(ie_fps) + '(FPS)'\n",
    "        elif i == 0:\n",
    "            addon += '{:4.3f}'.format(tf_fps) + '(FPS)'\n",
    "\n",
    "        ax_plot.text(0.5, -0.5, titles[i] + addon,\n",
    "                     size=28, ha=\"center\",\n",
    "                     transform=ax_plot.transAxes)\n",
    "        ax_plot.imshow(cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(path_to_model: str):\n",
    "    \"\"\"\n",
    "    Creates in memory graph in TensorFlow\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    graph = tf.Graph()\n",
    "    graph_def = tf.GraphDef()\n",
    "\n",
    "    with open(path_to_model, \"rb\") as model_file:\n",
    "        graph_def.ParseFromString(model_file.read())\n",
    "\n",
    "    nodes_to_clear_device = graph_def.node if isinstance(\n",
    "        graph_def, tf.GraphDef) else graph_def.graph_def.node\n",
    "    for node in nodes_to_clear_device:\n",
    "        node.device = \"\"\n",
    "\n",
    "    with graph.as_default():\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "    log.info(\"tf graph was created\")\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import graph_io\n",
    "\n",
    "import numpy as np\n",
    "import logging as log\n",
    "import time\n",
    "import os\n",
    "\n",
    "def children(op_name: str, graph: tf.Graph):\n",
    "    \"\"\"\n",
    "    Get operation node children.\n",
    "    \"\"\"\n",
    "    op = graph.get_operation_by_name(op_name)\n",
    "    return set(op for out in op.outputs for op in out.consumers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_graph(graph_def):\n",
    "    unlikely_output_types = [\n",
    "        'Const', 'Assign',\n",
    "        'NoOp', 'Placeholder',\n",
    "        'Assert', 'switch_t', 'switch_f'\n",
    "    ]\n",
    "    placeholders = dict()\n",
    "    outputs = list()\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():  # pylint: disable=not-context-manager\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "    for node in graph.as_graph_def().node:  # pylint: disable=no-member\n",
    "        if node.op == 'Placeholder':\n",
    "            node_dict = dict()\n",
    "            node_dict['type'] = tf.DType(node.attr['dtype'].type).name\n",
    "            new_shape = tf.TensorShape(node.attr['shape'].shape)\n",
    "            node_dict['shape'] = str(new_shape).replace(' ', '').replace('?', '-1')\n",
    "            placeholders[node.name] = node_dict\n",
    "        if len(children(node.name, graph)) == 0:\n",
    "            if node.op not in unlikely_output_types and \\\n",
    "                node.name.split('/')[-1] not in unlikely_output_types:\n",
    "                outputs.append(node.name)\n",
    "    result = dict()\n",
    "    result['inputs'] = placeholders\n",
    "    result['outputs'] = outputs\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_refs(graph: tf.Graph, input_data: dict):\n",
    "    \"\"\"\n",
    "    Return TensorFlow model reference results.\n",
    "    \"\"\"\n",
    "    log.info(\"Running inference with tensorflow ...\")\n",
    "    feed_dict = {}\n",
    "    summary_info = summarize_graph(graph.as_graph_def())\n",
    "    input_layers, output_layers = list(summary_info['inputs'].keys()), summary_info['outputs']\n",
    "\n",
    "    data_keys = [key for key in input_data.keys()]\n",
    "    if sorted(input_layers) != sorted(data_keys):\n",
    "        raise ValueError('input data keys: {0} do not match input '\n",
    "                         'layers of network: {1}'.format(data_keys, input_layers))\n",
    "\n",
    "    for input_layer_name in input_layers:\n",
    "        tensor = graph.get_tensor_by_name(input_layer_name + ':0')\n",
    "        feed_dict[tensor] = input_data[input_layer_name]\n",
    "    output_tensors = []\n",
    "    for name in output_layers:\n",
    "        tensor = graph.get_tensor_by_name(name + ':0')\n",
    "        output_tensors.append(tensor)\n",
    "\n",
    "    log.info(\"Running tf.Session\")\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # force inference on CPU\n",
    "    with graph.as_default():\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            inference_start = time.time()\n",
    "            outputs = session.run(output_tensors, feed_dict=feed_dict)\n",
    "            inference_end = time.time()\n",
    "    res = dict(zip(output_layers, outputs))\n",
    "    log.info(\"TensorFlow reference collected successfully\\n\")\n",
    "    return res, inference_end - inference_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_od_output(data: dict):\n",
    "    predictions = []\n",
    "    num_batches = len(data['detection_boxes'])\n",
    "    target_layers = ['num_detections', 'detection_classes',\n",
    "                     'detection_scores', 'detection_boxes']\n",
    "\n",
    "    for b in range(num_batches):\n",
    "        predictions.append([])\n",
    "        num_detections = int(data['num_detections'][b])\n",
    "        detection_classes = data['detection_classes'][b]\n",
    "        detection_scores = data['detection_scores'][b]\n",
    "        detection_boxes = data['detection_boxes'][b]\n",
    "        for i in range(num_detections):\n",
    "            obj = [\n",
    "                b, detection_classes[i], detection_scores[i],\n",
    "                detection_boxes[i][1], detection_boxes[i][0],\n",
    "                detection_boxes[i][3], detection_boxes[i][2]\n",
    "            ]\n",
    "            predictions[b].append(obj)\n",
    "    predictions = np.asarray(predictions)\n",
    "    new_shape = (1, 1, predictions.shape[0] * predictions.shape[1], predictions.shape[2])\n",
    "    predictions = np.reshape(predictions, newshape=new_shape)\n",
    "    parsed_data = {'tf_detections': predictions}\n",
    "    for layer, blob in data.items():\n",
    "        if layer not in target_layers:\n",
    "            parsed_data.update({layer: blob})\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_main(path_to_model: str, path_to_original_image: str, batch: int = 1):\n",
    "    \"\"\"\n",
    "    Entrypoint for inferencing with TensorFlow\n",
    "    \"\"\"\n",
    "    log.info('COMMON: image preprocessing')\n",
    "    width = 300\n",
    "    resized_image = read_resize_image(path_to_original_image, width, width)\n",
    "    \n",
    "    reshaped_image = np.reshape(resized_image, (width, width, 3))\n",
    "    batched_image = np.array([reshaped_image for _ in range(batch)])\n",
    "    \n",
    "    log.info('Current shape: {}'.format(batched_image.shape))\n",
    "\n",
    "    log.info('TENSORFLOW SPECIFIC: Loading a model with TensorFLow')\n",
    "    graph = load_graph(path_to_model)\n",
    "\n",
    "    input_data = {\n",
    "        'image_tensor': batched_image,\n",
    "    }\n",
    "\n",
    "    raw_results, delta = get_refs(graph, input_data)\n",
    "    log.info('TENSORFLOW SPECIFIC: Plain inference finished')\n",
    "\n",
    "    log.info('TENSORFLOW SPECIFIC: Post processing started')\n",
    "    processed_results = parse_od_output(raw_results)\n",
    "    log.info('TENSORFLOW SPECIFIC: Post processing finished')\n",
    "\n",
    "    return processed_results['tf_detections'], delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IENetwork, IEPlugin, IECore\n",
    "\n",
    "def ie_main(path_to_model_xml: str, path_to_model_bin: str, path_to_original_image: str, device='CPU', batch=1):\n",
    "    # First create Network (Note you need to provide model in IR previously converted with Model Optimizer)\n",
    "    log.info(\"Reading IR...\")\n",
    "    net = IENetwork(model=path_to_model_xml, weights=path_to_model_bin)\n",
    "\n",
    "    # Now let's create IECore() entity \n",
    "    log.info(\"Creating Inference Engine Core\")   \n",
    "    ie = IECore()\n",
    "\n",
    "\n",
    "    input_blob = next(iter(net.inputs))\n",
    "    out_blob = next(iter(net.outputs))\n",
    "\n",
    "    n, c, h, w = net.inputs[input_blob].shape\n",
    "    net.reshape({input_blob: (batch, c, h, w)})\n",
    "    n, c, h, w = net.inputs[input_blob].shape\n",
    "    \n",
    "    log.info('COMMON: image preprocessing')\n",
    "    image = read_resize_image(path_to_original_image, h, w)\n",
    "    # Now we load Network to plugin\n",
    "    log.info(\"Loading IR to the plugin...\")\n",
    "    exec_net = ie.load_network(network=net, device_name=device, num_requests=2)\n",
    "\n",
    "    del net\n",
    "\n",
    "    labels_map = None\n",
    "    \n",
    "    # Read and pre-process input image\n",
    "    image = image[..., ::-1]\n",
    "    in_frame = image.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "    batched_frame = np.array([in_frame for _ in range(batch)])\n",
    "    log.info('Current shape: {}'.format(batched_frame.shape))\n",
    "\n",
    "    # Now we run inference on target device\n",
    "    inference_start = time.time()\n",
    "    res = exec_net.infer(inputs={input_blob: batched_frame})\n",
    "    inference_end = time.time()\n",
    "\n",
    "    log.info('INFERENCE ENGINE SPECIFIC: no post processing')\n",
    "\n",
    "    return res[out_blob], inference_end - inference_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "\n",
    "log.basicConfig(format=\"[ %(levelname)s ] %(message)s\", level=log.INFO, stream=sys.stdout)\n",
    "\n",
    "NUM_RUNS = 1\n",
    "BATCH = 1\n",
    "\n",
    "DATA = os.path.join('.', 'workshop', 'data')\n",
    "\n",
    "IMAGE = os.path.join(DATA, 'images', 'input', 'dog.jpg')\n",
    "\n",
    "SSD_ASSETS = os.path.join(DATA, 'public', 'ssd_mobilenet_v2_coco')\n",
    "\n",
    "TF_MODEL = os.path.join(SSD_ASSETS, 'ssd_mobilenet_v2_coco_2018_03_29', 'frozen_inference_graph.pb')\n",
    "TF_RESULT_IMAGE = os.path.join(DATA, 'images', 'output', 'tensorflow_output.png')\n",
    "\n",
    "IE_MODEL_FP32_XML = os.path.join(SSD_ASSETS, 'FP32', 'ssd_mobilenet_v2_coco.xml')\n",
    "IE_MODEL_FP32_BIN = os.path.join(SSD_ASSETS, 'FP32', 'ssd_mobilenet_v2_coco.bin')\n",
    "IE_MODEL_FP16_XML = os.path.join(SSD_ASSETS, 'FP16', 'ssd_mobilenet_v2_coco.xml')\n",
    "IE_MODEL_FP16_BIN = os.path.join(SSD_ASSETS, 'FP16', 'ssd_mobilenet_v2_coco.bin')\n",
    "IE_MODEL_DEFAULT_INT8_XML = os.path.join(SSD_ASSETS, 'INT8', 'default', 'optimized', 'ssd_mobilenet_v2_coco.xml')\n",
    "IE_MODEL_DEFAULT_INT8_BIN = os.path.join(SSD_ASSETS, 'INT8', 'default', 'optimized', 'ssd_mobilenet_v2_coco.bin')\n",
    "IE_MODEL_AA_INT8_XML = os.path.join(SSD_ASSETS, 'INT8', 'acuracy_aware', 'optimized', 'ssd_mobilenet_v2_coco.xml')\n",
    "IE_MODEL_AA_INT8_BIN = os.path.join(SSD_ASSETS, 'INT8', 'acuracy_aware', 'optimized', 'ssd_mobilenet_v2_coco.bin')\n",
    "\n",
    "\n",
    "IE_RESULT_IMAGE = os.path.join(DATA, 'images', 'output', 'inference_engine_output.png')\n",
    "\n",
    "OPENVINO = os.getenv('INTEL_OPENVINO_DIR')\n",
    "if not OPENVINO:\n",
    "    print('Please, install OpenVINO and initialize the environment')\n",
    "    sys.exit(1)\n",
    "    \n",
    "if platform.system() == 'Linux':\n",
    "    ext = '.so'\n",
    "else:\n",
    "    print('You are running this demo on Windows OS or maxOS. However, this is demo for Linux.')\n",
    "    sys.exit(1)\n",
    "\n",
    "COMBO_RESULT_IMAGE = os.path.join(DATA, 'images', 'output', 'combo_output.png')\n",
    "\n",
    "PERFORMANCE = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_inference(xml:str, bin:str, device:str, postfix: str):\n",
    "    name = '{f} {p} on {d}'.format(f='IE', p=postfix, d=device)\n",
    "\n",
    "    ie_fps_collected = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        predictions, inf_time = ie_main(xml, bin,\n",
    "                                        IMAGE,\n",
    "                                        device,\n",
    "                                        batch=BATCH)\n",
    "        ie_fps = 1 / inf_time\n",
    "        ie_fps_collected.append(ie_fps)\n",
    "\n",
    "    ie_avg_fps = (sum(ie_fps_collected) * BATCH) / (NUM_RUNS)\n",
    "\n",
    "    PERFORMANCE[name] = ie_avg_fps\n",
    "\n",
    "    log.info('{} FPS: {}'.format(name, ie_avg_fps))\n",
    "\n",
    "    draw_image(IMAGE, predictions, IE_RESULT_IMAGE, color=(0, 0, 255))\n",
    "    \n",
    "    return ie_avg_fps, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework = 'TF'\n",
    "device = 'CPU'\n",
    "name = '{f} on {d}'.format(f=framework, d=device)\n",
    "\n",
    "tf_fps_collected = []\n",
    "\n",
    "for i in range(NUM_RUNS):\n",
    "    predictions, inf_time = tf_main(TF_MODEL, \n",
    "                                    IMAGE,\n",
    "                                    batch=BATCH)\n",
    "    tf_fps = 1 / inf_time\n",
    "    tf_fps_collected.append(tf_fps)\n",
    "\n",
    "tf_avg_fps = (sum(tf_fps_collected) * BATCH) / (NUM_RUNS)\n",
    "\n",
    "PERFORMANCE[name] = tf_avg_fps\n",
    "\n",
    "log.info('{} FPS: {}'.format(name, tf_avg_fps))\n",
    "\n",
    "draw_image(IMAGE, predictions, TF_RESULT_IMAGE, color=(255, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accuracy_check -c ./workshop/data/configs/accuracy_checker_config_tf.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'CPU'\n",
    "ie_avg_fps, predictions = ie_inference(IE_MODEL_FP32_XML, IE_MODEL_FP32_BIN, device, '')\n",
    "\n",
    "draw_image(TF_RESULT_IMAGE, predictions, COMBO_RESULT_IMAGE, color=(0, 0, 255))\n",
    "\n",
    "show_results_interactively(tf_image=TF_RESULT_IMAGE,\n",
    "                           ie_image=IE_RESULT_IMAGE,\n",
    "                           combination_image=COMBO_RESULT_IMAGE,\n",
    "                           ie_fps=ie_avg_fps,\n",
    "                           tf_fps=tf_avg_fps)\n",
    "\n",
    "show_performance(PERFORMANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:27:33 accuracy_checker WARNING: /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\n",
      "11:27:33 accuracy_checker WARNING: /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\n",
      "11:27:33 accuracy_checker WARNING: /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\n",
      "11:27:33 accuracy_checker WARNING: /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\n",
      "11:27:33 accuracy_checker WARNING: /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\n",
      "11:27:33 accuracy_checker WARNING: /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\n",
      "11:27:34 accuracy_checker WARNING: /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\n",
      "11:27:34 accuracy_checker WARNING: /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\n",
      "11:27:34 accuracy_checker WARNING: /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\n",
      "11:27:34 accuracy_checker WARNING: /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\n",
      "11:27:34 accuracy_checker WARNING: /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\n",
      "11:27:34 accuracy_checker WARNING: /home/atugarev/Developer/repositories/workbench/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\n",
      "Processing info:\n",
      "model: ssd_mobilenet_v1_coco\n",
      "launcher: dlsdk\n",
      "launcher tags: FP32\n",
      "device: CPU\n",
      "dataset: ms_coco_detection_91_classes\n",
      "OpenCV version: 4.2.0-openvino\n",
      "200it [00:00, 4548.42it/s]\n",
      "IE version: 2.1.37988\n",
      "Loaded CPU plugin version: 2.1.37988\n",
      "Input info:\n",
      "\tLayer name: image_tensor\n",
      "\tprecision: FP32\n",
      "\tshape [1, 3, 300, 300]\n",
      "\n",
      "Output info\n",
      "\tLayer name: DetectionOutput\n",
      "\tprecision: FP32\n",
      "\tshape [1, 1, 100, 7]\n",
      "\n",
      "200 objects processed in 6.232 seconds                                          \n",
      "coco_precision: 31.74%\n"
     ]
    }
   ],
   "source": [
    "!accuracy_check -c ./workshop/data/configs/accuracy_checker_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer/mo.py \\\n",
    "    --data_type FP16 \\\n",
    "    --output_dir=./workshop/data/public/ssd_mobilenet_v2_coco/FP16 \\\n",
    "    --reverse_input_channels \\\n",
    "    '--input_shape=[1,300,300,3]'\\\n",
    "    --model_name=ssd_mobilenet_v2_coco \\\n",
    "    --transformations_config=${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json \\\n",
    "    --tensorflow_object_detection_api_pipeline_config=./workshop/data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config \\\n",
    "    --output=detection_classes,detection_scores,detection_boxes,num_detections \\\n",
    "    --input_model=./workshop/data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'GPU'\n",
    "ie_avg_fps, predictions = ie_inference(IE_MODEL_FP16_XML, IE_MODEL_FP16_BIN, device, '')\n",
    "\n",
    "draw_image(TF_RESULT_IMAGE, predictions, COMBO_RESULT_IMAGE, color=(0, 0, 255))\n",
    "\n",
    "show_results_interactively(tf_image=TF_RESULT_IMAGE,\n",
    "                           ie_image=IE_RESULT_IMAGE,\n",
    "                           combination_image=COMBO_RESULT_IMAGE,\n",
    "                           ie_fps=ie_avg_fps,\n",
    "                           tf_fps=tf_avg_fps)\n",
    "\n",
    "show_performance(PERFORMANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/tools/post_training_optimization_toolkit/main.py -c ./workshop/data/configs/default/quantization_config.json \\\n",
    "                                                                                                 --output-dir ./workshop/data/public/ssd_mobilenet_v2_coco/INT8/default \\\n",
    "                                                                                                 --direct-dump \\\n",
    "                                                                                                 -e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'CPU'\n",
    "ie_avg_fps, predictions = ie_inference(IE_MODEL_DEFAULT_INT8_XML, IE_MODEL_DEFAULT_INT8_BIN, device, 'INT8 D')\n",
    "\n",
    "draw_image(TF_RESULT_IMAGE, predictions, COMBO_RESULT_IMAGE, color=(0, 0, 255))\n",
    "\n",
    "show_results_interactively(tf_image=TF_RESULT_IMAGE,\n",
    "                           ie_image=IE_RESULT_IMAGE,\n",
    "                           combination_image=COMBO_RESULT_IMAGE,\n",
    "                           ie_fps=ie_avg_fps,\n",
    "                           tf_fps=tf_avg_fps)\n",
    "\n",
    "show_performance(PERFORMANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accuracy_check -c ./workshop/data/configs/default/accuracy_checker_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/tools/post_training_optimization_toolkit/main.py \\\n",
    "    -c ./workshop/data/configs/accuracy_aware/quantization_config.json \\\n",
    "    --output-dir ./workshop/data/public/ssd_mobilenet_v2_coco/INT8/acuracy_aware \\\n",
    "    --direct-dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'CPU'\n",
    "ie_avg_fps, predictions = ie_inference(IE_MODEL_AA_INT8_XML, IE_MODEL_AA_INT8_BIN, device, 'INT8 AA')\n",
    "\n",
    "draw_image(TF_RESULT_IMAGE, predictions, COMBO_RESULT_IMAGE, color=(0, 0, 255))\n",
    "\n",
    "show_results_interactively(tf_image=TF_RESULT_IMAGE,\n",
    "                           ie_image=IE_RESULT_IMAGE,\n",
    "                           combination_image=COMBO_RESULT_IMAGE,\n",
    "                           ie_fps=ie_avg_fps,\n",
    "                           tf_fps=tf_avg_fps)\n",
    "\n",
    "show_performance(PERFORMANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accuracy_check -c ./workshop/data/configs/accuracy_aware/accuracy_checker_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/samples/cpp/build/intel64/Release/benchmark_app -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/samples/cpp/build/intel64/Release/benchmark_app -m workshop/data/public/ssd_mobilenet_v2_coco/FP32/ssd_mobilenet_v2_coco.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading models ||################\n",
      "\n",
      "========== Downloading workshop/data/parctice/public/alexnet/alexnet.prototxt\n",
      "... 100%, 3 KB, 8680 KB/s, 0 seconds passed\n",
      "\n",
      "========== Downloading workshop/data/parctice/public/alexnet/alexnet.caffemodel\n",
      "... 100%, 238146 KB, 9083 KB/s, 26 seconds passed\n",
      "\n",
      "################|| Post-processing ||################\n",
      "\n",
      "========== Replacing text in workshop/data/parctice/public/alexnet/alexnet.prototxt\n"
     ]
    }
   ],
   "source": [
    "!python3  ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/downloader.py \\\n",
    "                                                                                --name alexnet \\\n",
    "                                                                                --output_dir ./workshop/data/parctice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Converting alexnet to IR (FP32)\n",
      "Conversion command: /home/atugarev/Developer/repositories/workbench/venv/bin/python3 -- /home/atugarev/intel/openvino_2020.1.023/deployment_tools/model_optimizer/mo.py --framework=caffe --data_type=FP32 --output_dir=workshop/data/parctice/public/alexnet/FP32 --model_name=alexnet '--input_shape=[1,3,227,227]' --input=data '--mean_values=data[104.0,117.0,123.0]' --output=prob --input_model=workshop/data/parctice/public/alexnet/alexnet.caffemodel --input_proto=workshop/data/parctice/public/alexnet/alexnet.prototxt\n",
      "\n",
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/atugarev/workshop/data/parctice/public/alexnet/alexnet.caffemodel\n",
      "\t- Path for generated IR: \t/home/atugarev/workshop/data/parctice/public/alexnet/FP32\n",
      "\t- IR output name: \talexnet\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tdata\n",
      "\t- Output layers: \tprob\n",
      "\t- Input shapes: \t[1,3,227,227]\n",
      "\t- Mean values: \tdata[104.0,117.0,123.0]\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "Caffe specific parameters:\n",
      "\t- Path to Python Caffe* parser generated from caffe.proto: \t/home/atugarev/intel/openvino_2020.1.023/deployment_tools/model_optimizer/mo/front/caffe/proto\n",
      "\t- Enable resnet optimization: \tTrue\n",
      "\t- Path to the Input prototxt: \t/home/atugarev/workshop/data/parctice/public/alexnet/alexnet.prototxt\n",
      "\t- Path to CustomLayersMapping.xml: \tDefault\n",
      "\t- Path to a mean file: \tNot specified\n",
      "\t- Offsets for a mean file: \tNot specified\n",
      "Model Optimizer version: \t2020.1.0-61-gd349c3ba4a\n",
      "\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /home/atugarev/workshop/data/parctice/public/alexnet/FP32/alexnet.xml\n",
      "[ SUCCESS ] BIN file: /home/atugarev/workshop/data/parctice/public/alexnet/FP32/alexnet.bin\n",
      "[ SUCCESS ] Total execution time: 13.63 seconds. \n",
      "[ SUCCESS ] Memory consumed: 1482 MB. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/converter.py \\\n",
    "                                                         --name alexnet \\\n",
    "                                                         --download_dir ./workshop/data/parctice \\\n",
    "                                                         --output_dir ./workshop/data/parctice \\\n",
    "                                                         --precisions FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] Reading IR...\n",
      "[ INFO ] Creating Inference Engine Core\n",
      "[ INFO ] COMMON: image preprocessing\n",
      "[ INFO ] Loading IR to the plugin...\n",
      "[ INFO ] Current shape: (1, 3, 227, 227)\n",
      "[ INFO ] INFERENCE ENGINE SPECIFIC: no post processing\n"
     ]
    }
   ],
   "source": [
    " predictions = ie_main('workshop/data/parctice/public/alexnet/FP32/alexnet.xml', \n",
    "         'workshop/data/parctice/public/alexnet/FP32/alexnet.bin',\n",
    "                                        IMAGE,\n",
    "                                        'CPU',\n",
    "                                        batch=BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([248, 250, 249, 383, 166, 355, 162, 164, 384, 227])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "probs = np.squeeze(predictions[0])\n",
    "top_ind = np.argsort(probs)[-10:][::-1]\n",
    "top_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
